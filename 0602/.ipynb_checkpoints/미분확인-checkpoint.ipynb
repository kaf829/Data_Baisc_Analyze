{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6620586-8fd0-4dc0-b481-c25488c10a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 독립변수가 1개인 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18af8741-1107-4f3a-bb86-ee4b8dfef6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innital loss value =  12.821644490993595 Inital W ==  [[0.15601864]] \n",
      " , = b  [0.15599452]\n",
      "step =  0 loss value =  7.569736900943366 W -  [[0.39233487]] b =  [0.20933454]\n",
      "step =  300 loss value =  0.008943314794172489 W -  [[1.06141097]] b =  [0.77834116]\n",
      "step =  600 loss value =  0.0011353877598566174 W -  [[1.02188108]] b =  [0.92102168]\n",
      "step =  900 loss value =  0.0001441417857808511 W -  [[1.00779636]] b =  [0.97185957]\n",
      "step =  1200 loss value =  1.8299346833477664e-05 W -  [[1.00277789]] b =  [0.9899734]\n",
      "step =  1500 loss value =  2.323171540561446e-06 W -  [[1.00098978]] b =  [0.99642747]\n",
      "step =  1800 loss value =  2.949354452913258e-07 W -  [[1.00035266]] b =  [0.99872709]\n",
      "step =  2100 loss value =  3.7443174285855636e-08 W -  [[1.00012566]] b =  [0.99954645]\n",
      "step =  2400 loss value =  4.753553101139554e-09 W -  [[1.00004477]] b =  [0.9998384]\n",
      "step =  2700 loss value =  6.034816095812328e-10 W -  [[1.00001595]] b =  [0.99994242]\n",
      "step =  3000 loss value =  7.661428100889549e-11 W -  [[1.00000568]] b =  [0.99997948]\n",
      "step =  3300 loss value =  9.72647378254525e-12 W -  [[1.00000203]] b =  [0.99999269]\n",
      "step =  3600 loss value =  1.234812766790828e-12 W -  [[1.00000072]] b =  [0.9999974]\n",
      "step =  3900 loss value =  1.567641678347657e-13 W -  [[1.00000026]] b =  [0.99999907]\n",
      "step =  4200 loss value =  1.9901806089676195e-14 W -  [[1.00000009]] b =  [0.99999967]\n",
      "step =  4500 loss value =  2.5266098497646405e-15 W -  [[1.00000003]] b =  [0.99999988]\n",
      "step =  4800 loss value =  3.2076271451312132e-16 W -  [[1.00000001]] b =  [0.99999996]\n",
      "step =  5100 loss value =  4.0722039223415886e-17 W -  [[1.]] b =  [0.99999999]\n",
      "step =  5400 loss value =  5.169816654602417e-18 W -  [[1.]] b =  [0.99999999]\n",
      "step =  5700 loss value =  6.563277943873152e-19 W -  [[1.]] b =  [1.]\n",
      "step =  6000 loss value =  8.332353486748092e-20 W -  [[1.]] b =  [1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 입력 데이터와 정답 데이터\n",
    "x_data = np.array([1, 2, 3, 4, 5]).reshape(5, 1)\n",
    "t_data = np.array([2, 3, 4, 5, 6]).reshape(5, 1)\n",
    "\n",
    "# 가중치와 편향 초기화 (오타 수정: np.random.range → np.random.rand)\n",
    "w = np.random.rand(1, 1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "\n",
    "def loss_function(x, t):\n",
    "    y = np.dot(x, w) + b\n",
    "    return np.sum((t - y) ** 2) / len(x)\n",
    "\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4  # 오타: delt_x → delta_x\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:  \n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x)\n",
    "\n",
    "        x[idx] = float(tmp_val) - delta_x \n",
    "        fx2 = f(x)\n",
    "\n",
    "        grad[idx] = (fx1 - fx2) / (2 * delta_x)\n",
    "        x[idx] = tmp_val  # 원래 값 복원\n",
    "        it.iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "f = lambda x: loss_function(x_data, t_data)\n",
    "print(\"Innital loss value = \" , loss_function(x_data, t_data), \"Inital W == \", w, \"\\n\", \", = b \", b)\n",
    "\n",
    "for step in range(6001):\n",
    "    w -= learning_rate * numerical_derivative(f,w)\n",
    "    b -= learning_rate * numerical_derivative(f,b)\n",
    "\n",
    "    if(step % 300 == 0):\n",
    "        print(\"step = \" , step , \"loss value = \", loss_function(x_data, t_data), \"W - \", w, \"b = \" , b)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2dcd75e7-1965-4cba-8f43-c70e408840cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innital loss value =  8.332353486748092e-20 Inital W ==  [[1.]] \n",
      " , = b  [1.]\n",
      "step =  0 loss value =  8.275221577292356e-20 W -  [[1.]] b =  [1.]\n",
      "step =  300 loss value =  1.0505734280607197e-20 W -  [[1.]] b =  [1.]\n",
      "step =  600 loss value =  1.3337601753950156e-21 W -  [[1.]] b =  [1.]\n",
      "step =  900 loss value =  1.6932535705134757e-22 W -  [[1.]] b =  [1.]\n",
      "step =  1200 loss value =  2.1495893699016121e-23 W -  [[1.]] b =  [1.]\n",
      "step =  1500 loss value =  2.7309369390719493e-24 W -  [[1.]] b =  [1.]\n",
      "step =  1800 loss value =  3.4779859680626674e-25 W -  [[1.]] b =  [1.]\n",
      "step =  2100 loss value =  4.442250292774798e-26 W -  [[1.]] b =  [1.]\n",
      "step =  2400 loss value =  5.665125704754174e-27 W -  [[1.]] b =  [1.]\n",
      "step =  2700 loss value =  7.477218090137361e-28 W -  [[1.]] b =  [1.]\n",
      "step =  3000 loss value =  1.135959703518257e-28 W -  [[1.]] b =  [1.]\n",
      "step =  3300 loss value =  9.517606821491508e-29 W -  [[1.]] b =  [1.]\n",
      "step =  3600 loss value =  9.517606821491508e-29 W -  [[1.]] b =  [1.]\n",
      "step =  3900 loss value =  9.517606821491508e-29 W -  [[1.]] b =  [1.]\n",
      "step =  4200 loss value =  9.517606821491508e-29 W -  [[1.]] b =  [1.]\n",
      "step =  4500 loss value =  9.517606821491508e-29 W -  [[1.]] b =  [1.]\n",
      "step =  4800 loss value =  9.517606821491508e-29 W -  [[1.]] b =  [1.]\n",
      "step =  5100 loss value =  9.517606821491508e-29 W -  [[1.]] b =  [1.]\n",
      "step =  5400 loss value =  9.517606821491508e-29 W -  [[1.]] b =  [1.]\n",
      "step =  5700 loss value =  9.517606821491508e-29 W -  [[1.]] b =  [1.]\n",
      "step =  6000 loss value =  9.517606821491508e-29 W -  [[1.]] b =  [1.]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "f = lambda x: loss_function(x_data, t_data)\n",
    "print(\"Innital loss value = \" , loss_function(x_data, t_data), \"Inital W == \", w, \"\\n\", \", = b \", b)\n",
    "\n",
    "for step in range(6001):\n",
    "    w -= learning_rate * numerical_derivative(f,w)\n",
    "    b -= learning_rate * numerical_derivative(f,b)\n",
    "\n",
    "    if(step % 300 == 0):\n",
    "        print(\"step = \" , step , \"loss value = \", loss_function(x_data, t_data), \"W - \", w, \"b = \" , b)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a43413d-dd9e-4aa4-9187-7a2ca7457367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value = 10.533485410551242 \n",
      "Initial W = [[0.05808361]] , b = [0.86617615]\n",
      "step = 0, loss = 6.197838634406956, W = [[0.27333465]], b = [0.91245254]\n",
      "step = 300, loss = 9.932366687817541e-05, W = [[0.99352823]], b = [1.02335943]\n",
      "step = 600, loss = 1.2609516519650695e-05, W = [[0.99769407]], b = [1.0083231]\n",
      "step = 900, loss = 1.6008259849513681e-06, W = [[0.99917838]], b = [1.00296557]\n",
      "step = 1200, loss = 2.0323093515146146e-07, W = [[0.99970725]], b = [1.00105665]\n",
      "step = 1500, loss = 2.580093863483346e-08, W = [[0.99989569]], b = [1.00037649]\n",
      "step = 1800, loss = 3.2755270940949e-09, W = [[0.99996283]], b = [1.00013415]\n",
      "step = 2100, loss = 4.158405977421252e-10, W = [[0.99998676]], b = [1.0000478]\n",
      "step = 2400, loss = 5.27925423115133e-11, W = [[0.99999528]], b = [1.00001703]\n",
      "step = 2700, loss = 6.702213635098077e-12, W = [[0.99999832]], b = [1.00000607]\n",
      "step = 3000, loss = 8.508714612450469e-13, W = [[0.9999994]], b = [1.00000216]\n",
      "step = 3300, loss = 1.0802136178618564e-13, W = [[0.99999979]], b = [1.00000077]\n",
      "step = 3600, loss = 1.3713721997871863e-14, W = [[0.99999992]], b = [1.00000027]\n",
      "step = 3900, loss = 1.7410090682216356e-15, W = [[0.99999997]], b = [1.0000001]\n",
      "step = 4200, loss = 2.2102771817907852e-16, W = [[0.99999999]], b = [1.00000003]\n",
      "step = 4500, loss = 2.80603137289321e-17, W = [[1.]], b = [1.00000001]\n",
      "step = 4800, loss = 3.562363556728931e-18, W = [[1.]], b = [1.]\n",
      "step = 5100, loss = 4.522558531048161e-19, W = [[1.]], b = [1.]\n",
      "step = 5400, loss = 5.741556242405919e-20, W = [[1.]], b = [1.]\n",
      "step = 5700, loss = 7.289237215946083e-21, W = [[1.]], b = [1.]\n",
      "step = 6000, loss = 9.254464997600964e-22, W = [[1.]], b = [1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # 넘파이 모듈 불러오기 (수치 계산용)\n",
    "\n",
    "# 입력 데이터(x)와 정답 데이터(t)를 2차원 배열로 정의 (5행 1열)\n",
    "x_data = np.array([1, 2, 3, 4, 5]).reshape(5, 1)\n",
    "t_data = np.array([2, 3, 4, 5, 6]).reshape(5, 1)\n",
    "\n",
    "# 가중치(w)와 편향(b)를 0~1 사이의 난수로 초기화\n",
    "w = np.random.rand(1, 1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "# 손실 함수 정의: 평균 제곱 오차(MSE)\n",
    "def loss_function(x, t):\n",
    "    y = np.dot(x, w) + b  # 예측값 y = x*w + b\n",
    "    return np.sum((t - y) ** 2) / len(x)  # 오차 제곱의 평균 반환\n",
    "\n",
    "# 수치 미분 함수: f(x)에 대한 x의 미분(기울기)을 계산\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4  # 아주 작은 변화량\n",
    "    grad = np.zeros_like(x)  # 기울기 저장용 배열\n",
    "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=['readwrite'])  # 반복자 생성\n",
    "\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x)\n",
    "\n",
    "        x[idx] = float(tmp_val) - delta_x\n",
    "        fx2 = f(x)\n",
    "\n",
    "        grad[idx] = (fx1 - fx2) / (2 * delta_x)  # 중앙 차분법\n",
    "        x[idx] = tmp_val  # 원래 값 복원\n",
    "        it.iternext()\n",
    "\n",
    "    return grad\n",
    "\n",
    "# 학습률 설정\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# 손실 함수에 대한 람다 함수 정의 (x와 t는 고정되어 있고 w, b에 대해만 미분)\n",
    "f = lambda x: loss_function(x_data, t_data)\n",
    "\n",
    "# 초기 상태 출력\n",
    "print(\"Initial loss value =\", loss_function(x_data, t_data), \"\\nInitial W =\", w, \", b =\", b)\n",
    "\n",
    "# 경사하강법 학습 루프 (6001번 반복)\n",
    "for step in range(6001):\n",
    "    w -= learning_rate * numerical_derivative(f, w)  # w 업데이트\n",
    "    b -= learning_rate * numerical_derivative(f, b)  # b 업데이트\n",
    "\n",
    "    # 중간 출력 (300번마다)\n",
    "    if step % 300 == 0:\n",
    "        print(f\"step = {step}, loss = {loss_function(x_data, t_data)}, W = {w}, b = {b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "946e29fe-5c03-45c6-9c12-3d31998a3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    return np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ad4a3b2-8f60-4ad5-af5d-56c1e7fc66ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(np.array([43]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4e03d41-8290-464f-afc0-1ab0867382bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립변수가 2개인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd6ac6e8-b555-4d2d-8ca6-fe163e69be69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (9, 3)\n",
      "2 (9, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# CSV 파일에서 데이터 읽어오기 (delimiter 오타 수정)\n",
    "loaded_data = np.loadtxt(\"./data_01.csv\", delimiter=\",\", dtype=np.float32, skiprows=0)\n",
    "\n",
    "# 입력 데이터: 마지막 열 제외\n",
    "x_data = loaded_data[:, 0:-1]\n",
    "\n",
    "# 타겟 데이터: 마지막 열만 추출\n",
    "t_data = loaded_data[:, [-1]]\n",
    "\n",
    "# 차원 수와 모양 출력\n",
    "print(x_data.ndim, x_data.shape)\n",
    "print(t_data.ndim, t_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0b78d28-6cc4-4e55-9d38-24572ee1b289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37454012]\n",
      " [0.95071431]\n",
      " [0.73199394]] shape (3, 1) B =  [0.59865848] b shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "W= np.random.rand(3,1)\n",
    "b = np.random.rand(1)\n",
    "print(W, \"shape\" , W.shape,\"B = \",  b, \"b shape = \",  b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21feebe5-60de-454c-855e-84eaef4d2957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value = 2975.957435577398\n",
      "step = 0 loss value = 1202.1814172947873 W = [[0.68514971 0.79176209 0.10803839]] b = [0.97059173]\n",
      "step = 1000 loss value = 9.50784954117163 W = [[0.76695757 0.87329082 0.38119917]] b = [0.97654052]\n",
      "step = 2000 loss value = 7.855733889356376 W = [[0.71284125 0.81982802 0.48455536]] b = [0.98042193]\n",
      "step = 3000 loss value = 6.6717577935417625 W = [[0.66689793 0.77469885 0.57205539]] b = [0.98354399]\n",
      "step = 4000 loss value = 5.823253166121392 W = [[0.62791491 0.73658229 0.64613297]] b = [0.98602312]\n",
      "step = 5000 loss value = 5.215153608570723 W = [[0.59485223 0.70437364 0.70884816]] b = [0.98795796]\n",
      "step = 6000 loss value = 4.779334766228295 W = [[0.56682049 0.67714689 0.76194481]] b = [0.98943202]\n",
      "step = 7000 loss value = 4.466977821205344 W = [[0.54306062 0.65412438 0.80689906]] b = [0.99051605]\n",
      "step = 8000 loss value = 4.243098068389049 W = [[0.52292587 0.63465203 0.8449605 ]] b = [0.99126994]\n",
      "step = 9000 loss value = 4.082624424787918 W = [[0.50586591 0.61817899 0.87718693]] b = [0.99174442]\n",
      "step = 10000 loss value = 3.967590100537978 W = [[0.49141295 0.6042409  0.90447381]] b = [0.99198242]\n",
      "step = 11000 loss value = 3.885119435662582 W = [[0.47916969 0.59244598 0.92757914]] b = [0.99202031]\n",
      "step = 12000 loss value = 3.8259852383432738 W = [[0.46879889 0.58246344 0.94714462]] b = [0.99188888]\n",
      "step = 13000 loss value = 3.783574938819958 W = [[0.46001449 0.5740139  0.96371347]] b = [0.99161418]\n",
      "step = 14000 loss value = 3.7531497039819275 W = [[0.4525739  0.56686125 0.97774554]] b = [0.99121828]\n",
      "step = 15000 loss value = 3.731313493499828 W = [[0.4462715  0.56080587 0.98963011]] b = [0.99071987]\n",
      "step = 16000 loss value = 3.7156325658985443 W = [[0.44093302 0.55567899 0.99969672]] b = [0.99013476]\n",
      "step = 17000 loss value = 3.7043628019707473 W = [[0.43641081 0.55133786 1.00822435]] b = [0.98947633]\n",
      "step = 18000 loss value = 3.6962542939567857 W = [[0.4325798  0.54766172 1.01544915]] b = [0.98875592]\n",
      "step = 19000 loss value = 3.690411306293724 W = [[0.42933405 0.5445484  1.02157105]] b = [0.98798313]\n",
      "step = 20000 loss value = 3.6861919177959397 W = [[0.42658382 0.54191144 1.02675928]] b = [0.98716608]\n",
      "step = 21000 loss value = 3.6831361011349975 W = [[0.42425314 0.5396777  1.03115713]] b = [0.98631164]\n",
      "step = 22000 loss value = 3.6809141816379563 W = [[0.42227767 0.53778526 1.03488587]] b = [0.98542564]\n",
      "step = 23000 loss value = 3.6792899007378406 W = [[0.42060292 0.53618171 1.03804818]] b = [0.98451301]\n",
      "step = 24000 loss value = 3.678093945716656 W = [[0.41918277 0.53482272 1.04073098]] b = [0.98357791]\n",
      "step = 25000 loss value = 3.6772049800246953 W = [[0.41797816 0.53367073 1.04300785]] b = [0.98262387]\n",
      "step = 26000 loss value = 3.6765360488224426 W = [[0.41695603 0.53269399 1.04494107]] b = [0.9816539]\n",
      "step = 27000 loss value = 3.676024836628841 W = [[0.41608838 0.53186558 1.04658338]] b = [0.98067051]\n",
      "step = 28000 loss value = 3.6756266855488713 W = [[0.41535151 0.53116275 1.04797941]] b = [0.97967586]\n",
      "step = 29000 loss value = 3.675309591845713 W = [[0.41472535 0.53056623 1.04916695]] b = [0.97867175]\n",
      "step = 30000 loss value = 3.67505062027731 W = [[0.41419293 0.5300597  1.050178  ]] b = [0.97765972]\n",
      "step = 31000 loss value = 3.674833334461355 W = [[0.41373985 0.52962934 1.05103964]] b = [0.97664107]\n",
      "step = 32000 loss value = 3.6746459553679958 W = [[0.41335394 0.52926348 1.05177478]] b = [0.9756169]\n",
      "step = 33000 loss value = 3.6744800416198635 W = [[0.4130249  0.52895221 1.05240284]] b = [0.97458814]\n",
      "step = 34000 loss value = 3.674329543739747 W = [[0.41274401 0.52868716 1.05294025]] b = [0.97355558]\n",
      "step = 35000 loss value = 3.674190126385696 W = [[0.41250387 0.52846123 1.05340091]] b = [0.97251988]\n",
      "step = 36000 loss value = 3.6740586826364017 W = [[0.41229823 0.52826843 1.0537966 ]] b = [0.97148162]\n",
      "step = 37000 loss value = 3.673932985907819 W = [[0.41212181 0.52810366 1.05413729]] b = [0.97044127]\n",
      "step = 38000 loss value = 3.6738114405024027 W = [[0.41197011 0.52796263 1.05443141]] b = [0.96939923]\n",
      "step = 39000 loss value = 3.6736929028423098 W = [[0.41183935 0.5278417  1.0546861 ]] b = [0.96835585]\n",
      "step = 40000 loss value = 3.6735765533572513 W = [[0.41172632 0.52773779 1.05490742]] b = [0.96731141]\n",
      "step = 41000 loss value = 3.6734618046747056 W = [[0.41162829 0.52764827 1.05510048]] b = [0.96626617]\n",
      "step = 42000 loss value = 3.673348235824515 W = [[0.41154296 0.52757095 1.05526961]] b = [0.96522033]\n",
      "step = 43000 loss value = 3.673235545087723 W = [[0.41146839 0.52750396 1.05541849]] b = [0.96417406]\n",
      "step = 44000 loss value = 3.673123516205694 W = [[0.41140293 0.5274457  1.05555022]] b = [0.96312752]\n",
      "step = 45000 loss value = 3.673011994164287 W = [[0.41134517 0.52739484 1.05566742]] b = [0.96208083]\n",
      "step = 46000 loss value = 3.6729008678398194 W = [[0.41129395 0.52735025 1.05577233]] b = [0.96103409]\n",
      "step = 47000 loss value = 3.672790057562469 W = [[0.41124825 0.52731096 1.05586683]] b = [0.95998741]\n",
      "step = 48000 loss value = 3.6726795062035538 W = [[0.41120724 0.52727616 1.05595251]] b = [0.95894085]\n",
      "step = 49000 loss value = 3.672569172788346 W = [[0.41117018 0.52724517 1.05603072]] b = [0.95789447]\n",
      "step = 50000 loss value = 3.6724590279189004 W = [[0.41113649 0.52721739 1.0561026 ]] b = [0.95684834]\n",
      "step = 51000 loss value = 3.672349050493544 W = [[0.41110564 0.52719235 1.05616913]] b = [0.9558025]\n",
      "step = 52000 loss value = 3.672239225356358 W = [[0.41107719 0.52716961 1.05623112]] b = [0.95475698]\n",
      "step = 53000 loss value = 3.6721295416121214 W = [[0.41105079 0.52714883 1.05628927]] b = [0.95371183]\n",
      "step = 54000 loss value = 3.672019991419407 W = [[0.41102611 0.5271297  1.05634416]] b = [0.95266706]\n",
      "step = 55000 loss value = 3.671910569125058 W = [[0.41100289 0.52711198 1.05639628]] b = [0.95162271]\n",
      "step = 56000 loss value = 3.67180127064429 W = [[0.41098092 0.52709544 1.05644607]] b = [0.95057879]\n",
      "step = 57000 loss value = 3.6716920930162056 W = [[0.41096    0.52707991 1.05649388]] b = [0.94953531]\n",
      "step = 58000 loss value = 3.6715830340855677 W = [[0.41093997 0.52706524 1.05654   ]] b = [0.94849231]\n",
      "step = 59000 loss value = 3.671474092274307 W = [[0.41092069 0.52705128 1.05658469]] b = [0.94744977]\n",
      "step = 60000 loss value = 3.6713652664182863 W = [[0.41090206 0.52703794 1.05662818]] b = [0.94640773]\n",
      "step = 61000 loss value = 3.671256555649749 W = [[0.41088397 0.52702512 1.05667063]] b = [0.94536617]\n",
      "step = 62000 loss value = 3.6711479593136715 W = [[0.41086634 0.52701275 1.05671222]] b = [0.94432512]\n",
      "step = 63000 loss value = 3.6710394769072727 W = [[0.4108491  0.52700074 1.05675306]] b = [0.94328457]\n",
      "step = 64000 loss value = 3.670931108037073 W = [[0.4108322  0.52698905 1.05679326]] b = [0.94224454]\n",
      "step = 65000 loss value = 3.6708228523879307 W = [[0.41081558 0.52697763 1.05683294]] b = [0.94120502]\n",
      "step = 66000 loss value = 3.6707147097007766 W = [[0.4107992  0.52696644 1.05687215]] b = [0.94016602]\n",
      "step = 67000 loss value = 3.670606679756885 W = [[0.41078302 0.52695545 1.05691097]] b = [0.93912755]\n",
      "step = 68000 loss value = 3.6704987623664644 W = [[0.41076702 0.52694462 1.05694946]] b = [0.9380896]\n",
      "step = 69000 loss value = 3.670390957360427 W = [[0.41075117 0.52693393 1.05698767]] b = [0.93705217]\n",
      "step = 70000 loss value = 3.670283264584576 W = [[0.41073545 0.52692336 1.05702563]] b = [0.93601528]\n",
      "step = 71000 loss value = 3.6701756838955046 W = [[0.41071983 0.52691289 1.05706338]] b = [0.93497892]\n",
      "step = 72000 loss value = 3.6700682151574324 W = [[0.41070431 0.52690251 1.05710095]] b = [0.93394308]\n",
      "step = 73000 loss value = 3.669960858240195 W = [[0.41068887 0.52689221 1.05713836]] b = [0.93290778]\n",
      "step = 74000 loss value = 3.6698536130176276 W = [[0.4106735  0.52688197 1.05717564]] b = [0.93187302]\n",
      "step = 75000 loss value = 3.669746479366528 W = [[0.41065819 0.52687178 1.0572128 ]] b = [0.93083878]\n",
      "step = 76000 loss value = 3.669639457165764 W = [[0.41064293 0.52686165 1.05724986]] b = [0.92980508]\n",
      "step = 77000 loss value = 3.669532546295829 W = [[0.41062771 0.52685155 1.05728683]] b = [0.92877192]\n",
      "step = 78000 loss value = 3.6694257466382334 W = [[0.41061254 0.52684149 1.05732373]] b = [0.92773929]\n",
      "step = 79000 loss value = 3.6693190580754997 W = [[0.4105974  0.52683146 1.05736055]] b = [0.92670719]\n",
      "step = 80000 loss value = 3.6692124804906996 W = [[0.41058228 0.52682146 1.05739731]] b = [0.92567563]\n",
      "step = 81000 loss value = 3.6691060137674754 W = [[0.4105672  0.52681148 1.05743402]] b = [0.92464461]\n",
      "step = 82000 loss value = 3.6689996577897768 W = [[0.41055214 0.52680152 1.05747068]] b = [0.92361412]\n",
      "step = 83000 loss value = 3.6688934124419483 W = [[0.4105371  0.52679157 1.05750729]] b = [0.92258416]\n",
      "step = 84000 loss value = 3.668787277608552 W = [[0.41052209 0.52678165 1.05754386]] b = [0.92155474]\n",
      "step = 85000 loss value = 3.668681253174369 W = [[0.41050708 0.52677174 1.05758039]] b = [0.92052585]\n",
      "step = 86000 loss value = 3.6685753390244407 W = [[0.4104921  0.52676184 1.05761689]] b = [0.9194975]\n",
      "step = 87000 loss value = 3.668469535043829 W = [[0.41047713 0.52675196 1.05765336]] b = [0.91846969]\n",
      "step = 88000 loss value = 3.6683638411178685 W = [[0.41046217 0.52674208 1.0576898 ]] b = [0.9174424]\n",
      "step = 89000 loss value = 3.668258257132101 W = [[0.41044723 0.52673222 1.05772621]] b = [0.91641565]\n",
      "step = 90000 loss value = 3.6681527829720264 W = [[0.4104323  0.52672236 1.05776259]] b = [0.91538944]\n",
      "step = 91000 loss value = 3.6680474185234027 W = [[0.41041738 0.52671252 1.05779894]] b = [0.91436375]\n",
      "step = 92000 loss value = 3.6679421636720795 W = [[0.41040247 0.52670268 1.05783527]] b = [0.91333861]\n",
      "step = 93000 loss value = 3.6678370183040587 W = [[0.41038757 0.52669285 1.05787158]] b = [0.91231399]\n",
      "step = 94000 loss value = 3.6677319823054453 W = [[0.41037268 0.52668302 1.05790787]] b = [0.91128991]\n",
      "step = 95000 loss value = 3.6676270555624058 W = [[0.4103578  0.52667321 1.05794413]] b = [0.91026636]\n",
      "step = 96000 loss value = 3.6675222379613737 W = [[0.41034293 0.5266634  1.05798037]] b = [0.90924334]\n",
      "step = 97000 loss value = 3.6674175293887896 W = [[0.41032807 0.52665359 1.05801659]] b = [0.90822085]\n",
      "step = 98000 loss value = 3.667312929731181 W = [[0.41031322 0.5266438  1.05805279]] b = [0.9071989]\n",
      "step = 99000 loss value = 3.667208438875353 W = [[0.41029838 0.526634   1.05808896]] b = [0.90617748]\n",
      "step = 100000 loss value = 3.667104056708049 W = [[0.41028354 0.52662422 1.05812512]] b = [0.90515658]\n",
      "step = 101000 loss value = 3.666999783116277 W = [[0.41026871 0.52661444 1.05816126]] b = [0.90413622]\n",
      "step = 102000 loss value = 3.666895617987026 W = [[0.4102539  0.52660466 1.05819737]] b = [0.90311639]\n",
      "step = 103000 loss value = 3.6667915612075594 W = [[0.41023909 0.5265949  1.05823347]] b = [0.9020971]\n",
      "step = 104000 loss value = 3.6666876126650894 W = [[0.41022428 0.52658513 1.05826955]] b = [0.90107833]\n",
      "step = 105000 loss value = 3.6665837722471246 W = [[0.41020949 0.52657537 1.05830561]] b = [0.90006009]\n",
      "step = 106000 loss value = 3.666480039841122 W = [[0.4101947  0.52656562 1.05834165]] b = [0.89904238]\n",
      "step = 107000 loss value = 3.666376415334764 W = [[0.41017993 0.52655587 1.05837767]] b = [0.8980252]\n",
      "step = 108000 loss value = 3.666272898615796 W = [[0.41016516 0.52654613 1.05841367]] b = [0.89700855]\n",
      "step = 109000 loss value = 3.6661694895720833 W = [[0.41015039 0.5265364  1.05844965]] b = [0.89599243]\n",
      "step = 110000 loss value = 3.6660661880916905 W = [[0.41013564 0.52652666 1.05848561]] b = [0.89497683]\n",
      "step = 111000 loss value = 3.6659629940626934 W = [[0.41012089 0.52651694 1.05852156]] b = [0.89396177]\n",
      "step = 112000 loss value = 3.665859907373325 W = [[0.41010615 0.52650722 1.05855748]] b = [0.89294723]\n",
      "step = 113000 loss value = 3.665756927911916 W = [[0.41009142 0.5264975  1.05859339]] b = [0.89193322]\n",
      "step = 114000 loss value = 3.665654055566926 W = [[0.4100767  0.52648779 1.05862927]] b = [0.89091974]\n",
      "step = 115000 loss value = 3.6655512902270235 W = [[0.41006199 0.52647808 1.05866514]] b = [0.88990679]\n",
      "step = 116000 loss value = 3.665448631780828 W = [[0.41004728 0.52646838 1.05870099]] b = [0.88889436]\n",
      "step = 117000 loss value = 3.665346080117102 W = [[0.41003258 0.52645869 1.05873682]] b = [0.88788246]\n",
      "step = 118000 loss value = 3.6652436351248903 W = [[0.41001788 0.526449   1.05877263]] b = [0.88687108]\n",
      "step = 119000 loss value = 3.665141296693121 W = [[0.4100032  0.52643931 1.05880843]] b = [0.88586024]\n",
      "step = 120000 loss value = 3.665039064711052 W = [[0.40998852 0.52642963 1.0588442 ]] b = [0.88484991]\n",
      "step = 121000 loss value = 3.664936939067933 W = [[0.40997385 0.52641996 1.05887996]] b = [0.88384012]\n",
      "step = 122000 loss value = 3.6648349196530807 W = [[0.40995919 0.52641029 1.05891569]] b = [0.88283085]\n",
      "step = 123000 loss value = 3.664733006356128 W = [[0.40994454 0.52640062 1.05895141]] b = [0.8818221]\n",
      "step = 124000 loss value = 3.664631199066582 W = [[0.40992989 0.52639096 1.05898711]] b = [0.88081388]\n",
      "step = 125000 loss value = 3.664529497674199 W = [[0.40991525 0.52638131 1.0590228 ]] b = [0.87980618]\n",
      "step = 126000 loss value = 3.6644279020688746 W = [[0.40990062 0.52637166 1.05905846]] b = [0.87879901]\n",
      "step = 127000 loss value = 3.6643264121405528 W = [[0.409886   0.52636201 1.0590941 ]] b = [0.87779236]\n",
      "step = 128000 loss value = 3.6642250277792505 W = [[0.40987138 0.52635237 1.05912973]] b = [0.87678624]\n",
      "step = 129000 loss value = 3.6641237488752347 W = [[0.40985677 0.52634274 1.05916534]] b = [0.87578063]\n",
      "step = 130000 loss value = 3.6640225753188083 W = [[0.40984217 0.52633311 1.05920092]] b = [0.87477555]\n",
      "step = 131000 loss value = 3.6639215070003583 W = [[0.40982758 0.52632348 1.05923649]] b = [0.873771]\n",
      "step = 132000 loss value = 3.663820543810386 W = [[0.40981299 0.52631386 1.05927205]] b = [0.87276697]\n",
      "step = 133000 loss value = 3.663719685639624 W = [[0.40979842 0.52630425 1.05930758]] b = [0.87176345]\n",
      "step = 134000 loss value = 3.663618932378804 W = [[0.40978385 0.52629464 1.05934309]] b = [0.87076047]\n",
      "step = 135000 loss value = 3.6635182839187923 W = [[0.40976928 0.52628503 1.05937859]] b = [0.869758]\n",
      "step = 136000 loss value = 3.6634177401505763 W = [[0.40975473 0.52627543 1.05941407]] b = [0.86875605]\n",
      "step = 137000 loss value = 3.663317300965213 W = [[0.40974018 0.52626584 1.05944953]] b = [0.86775463]\n",
      "step = 138000 loss value = 3.663216966254006 W = [[0.40972564 0.52625625 1.05948497]] b = [0.86675373]\n",
      "step = 139000 loss value = 3.6631167359082193 W = [[0.40971111 0.52624666 1.05952039]] b = [0.86575334]\n",
      "step = 140000 loss value = 3.6630166098193473 W = [[0.40969658 0.52623708 1.05955579]] b = [0.86475348]\n",
      "step = 141000 loss value = 3.6629165878788825 W = [[0.40968206 0.52622751 1.05959118]] b = [0.86375414]\n",
      "step = 142000 loss value = 3.662816669978576 W = [[0.40966755 0.52621794 1.05962655]] b = [0.86275532]\n",
      "step = 143000 loss value = 3.662716856010111 W = [[0.40965305 0.52620837 1.0596619 ]] b = [0.86175701]\n",
      "step = 144000 loss value = 3.662617145865455 W = [[0.40963856 0.52619881 1.05969723]] b = [0.86075923]\n",
      "step = 145000 loss value = 3.662517539436564 W = [[0.40962407 0.52618926 1.05973254]] b = [0.85976197]\n",
      "step = 146000 loss value = 3.662418036615595 W = [[0.40960959 0.52617971 1.05976783]] b = [0.85876522]\n",
      "step = 147000 loss value = 3.6623186372947507 W = [[0.40959512 0.52617016 1.05980311]] b = [0.85776899]\n",
      "step = 148000 loss value = 3.6622193413664013 W = [[0.40958065 0.52616062 1.05983836]] b = [0.85677329]\n",
      "step = 149000 loss value = 3.6621201487229555 W = [[0.4095662  0.52615108 1.0598736 ]] b = [0.8557781]\n",
      "step = 150000 loss value = 3.662021059256998 W = [[0.40955175 0.52614155 1.05990882]] b = [0.85478342]\n",
      "step = 151000 loss value = 3.661922072861213 W = [[0.40953731 0.52613203 1.05994403]] b = [0.85378927]\n",
      "step = 152000 loss value = 3.661823189428422 W = [[0.40952287 0.52612251 1.05997921]] b = [0.85279563]\n",
      "step = 153000 loss value = 3.6617244088514784 W = [[0.40950844 0.52611299 1.06001437]] b = [0.85180251]\n",
      "step = 154000 loss value = 3.661625731023408 W = [[0.40949402 0.52610348 1.06004952]] b = [0.8508099]\n",
      "step = 155000 loss value = 3.6615271558373106 W = [[0.40947961 0.52609398 1.06008465]] b = [0.84981782]\n",
      "step = 156000 loss value = 3.661428683186493 W = [[0.40946521 0.52608448 1.06011976]] b = [0.84882624]\n",
      "step = 157000 loss value = 3.6613303129642603 W = [[0.40945081 0.52607498 1.06015485]] b = [0.84783519]\n",
      "step = 158000 loss value = 3.661232045064 W = [[0.40943642 0.52606549 1.06018993]] b = [0.84684465]\n",
      "step = 159000 loss value = 3.6611338793794235 W = [[0.40942204 0.526056   1.06022498]] b = [0.84585462]\n",
      "step = 160000 loss value = 3.661035815804069 W = [[0.40940766 0.52604652 1.06026002]] b = [0.84486511]\n",
      "step = 161000 loss value = 3.6609378542317748 W = [[0.4093933  0.52603705 1.06029504]] b = [0.84387611]\n",
      "step = 162000 loss value = 3.6608399945564636 W = [[0.40937894 0.52602758 1.06033004]] b = [0.84288763]\n",
      "step = 163000 loss value = 3.660742236672194 W = [[0.40936459 0.52601811 1.06036502]] b = [0.84189967]\n",
      "step = 164000 loss value = 3.660644580472961 W = [[0.40935024 0.52600865 1.06039999]] b = [0.84091221]\n",
      "step = 165000 loss value = 3.660547025853058 W = [[0.4093359  0.52599919 1.06043493]] b = [0.83992527]\n",
      "step = 166000 loss value = 3.660449572706858 W = [[0.40932157 0.52598974 1.06046986]] b = [0.83893885]\n",
      "step = 167000 loss value = 3.6603522209287456 W = [[0.40930725 0.5259803  1.06050477]] b = [0.83795294]\n",
      "step = 168000 loss value = 3.6602549704133094 W = [[0.40929294 0.52597085 1.06053966]] b = [0.83696754]\n",
      "step = 169000 loss value = 3.6601578210552606 W = [[0.40927863 0.52596142 1.06057454]] b = [0.83598265]\n",
      "step = 170000 loss value = 3.660060772749291 W = [[0.40926433 0.52595199 1.06060939]] b = [0.83499827]\n",
      "step = 171000 loss value = 3.659963825390325 W = [[0.40925004 0.52594256 1.06064423]] b = [0.83401441]\n",
      "step = 172000 loss value = 3.659866978873419 W = [[0.40923575 0.52593314 1.06067905]] b = [0.83303106]\n",
      "step = 173000 loss value = 3.6597702330936084 W = [[0.40922147 0.52592372 1.06071385]] b = [0.83204822]\n",
      "step = 174000 loss value = 3.6596735879461204 W = [[0.4092072  0.52591431 1.06074863]] b = [0.83106589]\n",
      "step = 175000 loss value = 3.659577043326302 W = [[0.40919294 0.5259049  1.0607834 ]] b = [0.83008408]\n",
      "step = 176000 loss value = 3.6594805991295822 W = [[0.40917869 0.5258955  1.06081815]] b = [0.82910277]\n",
      "step = 177000 loss value = 3.659384255251487 W = [[0.40916444 0.5258861  1.06085288]] b = [0.82812197]\n",
      "step = 178000 loss value = 3.65928801158768 W = [[0.4091502  0.52587671 1.06088759]] b = [0.82714169]\n",
      "step = 179000 loss value = 3.6591918680339606 W = [[0.40913596 0.52586732 1.06092228]] b = [0.82616191]\n",
      "step = 180000 loss value = 3.6590958244861156 W = [[0.40912174 0.52585794 1.06095695]] b = [0.82518265]\n",
      "step = 181000 loss value = 3.6589998808401436 W = [[0.40910752 0.52584856 1.06099161]] b = [0.82420389]\n",
      "step = 182000 loss value = 3.6589040369921944 W = [[0.40909331 0.52583919 1.06102625]] b = [0.82322564]\n",
      "step = 183000 loss value = 3.658808292838376 W = [[0.40907911 0.52582982 1.06106087]] b = [0.8222479]\n",
      "step = 184000 loss value = 3.6587126482750625 W = [[0.40906491 0.52582046 1.06109547]] b = [0.82127067]\n",
      "step = 185000 loss value = 3.6586171031986248 W = [[0.40905072 0.5258111  1.06113006]] b = [0.82029395]\n",
      "step = 186000 loss value = 3.6585216575055304 W = [[0.40903654 0.52580175 1.06116462]] b = [0.81931774]\n",
      "step = 187000 loss value = 3.6584263110925113 W = [[0.40902236 0.5257924  1.06119917]] b = [0.81834203]\n",
      "step = 188000 loss value = 3.6583310638562256 W = [[0.4090082  0.52578305 1.0612337 ]] b = [0.81736684]\n",
      "step = 189000 loss value = 3.6582359156935573 W = [[0.40899404 0.52577372 1.06126822]] b = [0.81639215]\n",
      "step = 190000 loss value = 3.658140866501379 W = [[0.40897989 0.52576438 1.06130271]] b = [0.81541796]\n",
      "step = 191000 loss value = 3.6580459161768317 W = [[0.40896574 0.52575505 1.06133719]] b = [0.81444429]\n",
      "step = 192000 loss value = 3.657951064617033 W = [[0.40895161 0.52574573 1.06137165]] b = [0.81347112]\n",
      "step = 193000 loss value = 3.657856311719213 W = [[0.40893748 0.52573641 1.06140609]] b = [0.81249845]\n",
      "step = 194000 loss value = 3.6577616573808154 W = [[0.40892335 0.52572709 1.06144051]] b = [0.81152629]\n",
      "step = 195000 loss value = 3.657667101499279 W = [[0.40890924 0.52571778 1.06147491]] b = [0.81055464]\n",
      "step = 196000 loss value = 3.6575726439722445 W = [[0.40889513 0.52570848 1.0615093 ]] b = [0.8095835]\n",
      "step = 197000 loss value = 3.6574782846973153 W = [[0.40888103 0.52569918 1.06154367]] b = [0.80861285]\n",
      "step = 198000 loss value = 3.657384023572347 W = [[0.40886694 0.52568988 1.06157802]] b = [0.80764272]\n",
      "step = 199000 loss value = 3.6572898604952604 W = [[0.40885285 0.52568059 1.06161236]] b = [0.80667309]\n",
      "step = 200000 loss value = 3.657195795364079 W = [[0.40883877 0.52567131 1.06164667]] b = [0.80570396]\n",
      "step = 201000 loss value = 3.657101828076855 W = [[0.4088247  0.52566203 1.06168097]] b = [0.80473534]\n",
      "step = 202000 loss value = 3.6570079585318687 W = [[0.40881064 0.52565275 1.06171525]] b = [0.80376722]\n",
      "step = 203000 loss value = 3.6569141866274126 W = [[0.40879658 0.52564348 1.06174951]] b = [0.8027996]\n",
      "step = 204000 loss value = 3.6568205122619712 W = [[0.40878253 0.52563422 1.06178376]] b = [0.80183249]\n",
      "step = 205000 loss value = 3.656726935334031 W = [[0.40876849 0.52562495 1.06181798]] b = [0.80086588]\n",
      "step = 206000 loss value = 3.656633455742318 W = [[0.40875445 0.5256157  1.06185219]] b = [0.79989977]\n",
      "step = 207000 loss value = 3.6565400733854982 W = [[0.40874043 0.52560645 1.06188638]] b = [0.79893417]\n",
      "step = 208000 loss value = 3.6564467881625102 W = [[0.40872641 0.5255972  1.06192056]] b = [0.79796907]\n",
      "step = 209000 loss value = 3.6563535999722347 W = [[0.4087124  0.52558796 1.06195471]] b = [0.79700447]\n",
      "step = 210000 loss value = 3.656260508713831 W = [[0.40869839 0.52557872 1.06198885]] b = [0.79604037]\n",
      "step = 211000 loss value = 3.656167514286416 W = [[0.40868439 0.52556949 1.06202297]] b = [0.79507677]\n",
      "step = 212000 loss value = 3.6560746165892724 W = [[0.4086704  0.52556026 1.06205707]] b = [0.79411368]\n",
      "step = 213000 loss value = 3.655981815521792 W = [[0.40865642 0.52555104 1.06209115]] b = [0.79315109]\n",
      "step = 214000 loss value = 3.6558891109834573 W = [[0.40864244 0.52554182 1.06212522]] b = [0.79218899]\n",
      "step = 215000 loss value = 3.655796502873871 W = [[0.40862847 0.5255326  1.06215927]] b = [0.7912274]\n",
      "step = 216000 loss value = 3.655703991092744 W = [[0.40861451 0.5255234  1.0621933 ]] b = [0.79026631]\n",
      "step = 217000 loss value = 3.6556115755398415 W = [[0.40860056 0.52551419 1.06222732]] b = [0.78930572]\n",
      "step = 218000 loss value = 3.6555192561150895 W = [[0.40858661 0.52550499 1.06226131]] b = [0.78834562]\n",
      "step = 219000 loss value = 3.6554270327184923 W = [[0.40857267 0.5254958  1.06229529]] b = [0.78738603]\n",
      "step = 220000 loss value = 3.6553349052502093 W = [[0.40855874 0.52548661 1.06232925]] b = [0.78642694]\n",
      "step = 221000 loss value = 3.6552428736103684 W = [[0.40854481 0.52547743 1.06236319]] b = [0.78546834]\n",
      "step = 222000 loss value = 3.6551509376993647 W = [[0.40853089 0.52546825 1.06239712]] b = [0.78451025]\n",
      "step = 223000 loss value = 3.655059097417609 W = [[0.40851698 0.52545907 1.06243102]] b = [0.78355265]\n",
      "step = 224000 loss value = 3.6549673526655977 W = [[0.40850308 0.5254499  1.06246491]] b = [0.78259555]\n",
      "step = 225000 loss value = 3.6548757033440173 W = [[0.40848918 0.52544073 1.06249879]] b = [0.78163895]\n",
      "step = 226000 loss value = 3.654784149353553 W = [[0.40847529 0.52543157 1.06253264]] b = [0.78068284]\n",
      "step = 227000 loss value = 3.6546926905950587 W = [[0.40846141 0.52542242 1.06256648]] b = [0.77972724]\n",
      "step = 228000 loss value = 3.654601326969503 W = [[0.40844753 0.52541327 1.0626003 ]] b = [0.77877213]\n",
      "step = 229000 loss value = 3.6545100583778987 W = [[0.40843367 0.52540412 1.0626341 ]] b = [0.77781751]\n",
      "step = 230000 loss value = 3.6544188847214354 W = [[0.40841981 0.52539498 1.06266788]] b = [0.7768634]\n",
      "step = 231000 loss value = 3.6543278059012625 W = [[0.40840595 0.52538584 1.06270165]] b = [0.77590978]\n",
      "step = 232000 loss value = 3.654236821818896 W = [[0.40839211 0.52537671 1.0627354 ]] b = [0.77495665]\n",
      "step = 233000 loss value = 3.6541459323756373 W = [[0.40837827 0.52536758 1.06276913]] b = [0.77400403]\n",
      "step = 234000 loss value = 3.654055137473169 W = [[0.40836444 0.52535846 1.06280284]] b = [0.77305189]\n",
      "step = 235000 loss value = 3.6539644370130415 W = [[0.40835061 0.52534934 1.06283654]] b = [0.77210026]\n",
      "step = 236000 loss value = 3.6538738308970977 W = [[0.4083368  0.52534023 1.06287022]] b = [0.77114912]\n",
      "step = 237000 loss value = 3.6537833190271516 W = [[0.40832299 0.52533112 1.06290388]] b = [0.77019847]\n",
      "step = 238000 loss value = 3.653692901305228 W = [[0.40830918 0.52532202 1.06293752]] b = [0.76924832]\n",
      "step = 239000 loss value = 3.6536025776333534 W = [[0.40829539 0.52531292 1.06297115]] b = [0.76829866]\n",
      "step = 240000 loss value = 3.6535123479137193 W = [[0.4082816  0.52530382 1.06300476]] b = [0.76734949]\n",
      "step = 241000 loss value = 3.6534222120485476 W = [[0.40826782 0.52529473 1.06303835]] b = [0.76640082]\n",
      "step = 242000 loss value = 3.6533321699402888 W = [[0.40825404 0.52528565 1.06307193]] b = [0.76545265]\n",
      "step = 243000 loss value = 3.6532422214913924 W = [[0.40824028 0.52527657 1.06310548]] b = [0.76450496]\n",
      "step = 244000 loss value = 3.653152366604422 W = [[0.40822652 0.52526749 1.06313902]] b = [0.76355777]\n",
      "step = 245000 loss value = 3.653062605182072 W = [[0.40821276 0.52525842 1.06317254]] b = [0.76261107]\n",
      "step = 246000 loss value = 3.652972937127108 W = [[0.40819902 0.52524936 1.06320605]] b = [0.76166487]\n",
      "step = 247000 loss value = 3.6528833623424593 W = [[0.40818528 0.5252403  1.06323953]] b = [0.76071916]\n",
      "step = 248000 loss value = 3.652793880731025 W = [[0.40817155 0.52523124 1.063273  ]] b = [0.75977394]\n",
      "step = 249000 loss value = 3.652704492195969 W = [[0.40815782 0.52522219 1.06330645]] b = [0.75882921]\n",
      "step = 250000 loss value = 3.6526151966404123 W = [[0.40814411 0.52521314 1.06333989]] b = [0.75788497]\n",
      "step = 251000 loss value = 3.652525993967698 W = [[0.4081304  0.5252041  1.06337331]] b = [0.75694122]\n",
      "step = 252000 loss value = 3.6524368840811516 W = [[0.4081167  0.52519506 1.0634067 ]] b = [0.75599797]\n",
      "step = 253000 loss value = 3.652347866884323 W = [[0.408103   0.52518603 1.06344009]] b = [0.7550552]\n",
      "step = 254000 loss value = 3.652258942280752 W = [[0.40808931 0.525177   1.06347345]] b = [0.75411293]\n",
      "step = 255000 loss value = 3.6521701101741266 W = [[0.40807563 0.52516798 1.0635068 ]] b = [0.75317114]\n",
      "step = 256000 loss value = 3.6520813704682547 W = [[0.40806196 0.52515896 1.06354013]] b = [0.75222985]\n",
      "step = 257000 loss value = 3.6519927230670044 W = [[0.40804829 0.52514994 1.06357344]] b = [0.75128904]\n",
      "step = 258000 loss value = 3.65190416787439 W = [[0.40803463 0.52514093 1.06360674]] b = [0.75034873]\n",
      "step = 259000 loss value = 3.651815704794508 W = [[0.40802098 0.52513193 1.06364002]] b = [0.7494089]\n",
      "step = 260000 loss value = 3.651727333731491 W = [[0.40800733 0.52512293 1.06367328]] b = [0.74846956]\n",
      "step = 261000 loss value = 3.6516390545896673 W = [[0.40799369 0.52511393 1.06370652]] b = [0.74753071]\n",
      "step = 262000 loss value = 3.651550867273384 W = [[0.40798006 0.52510494 1.06373975]] b = [0.74659235]\n",
      "step = 263000 loss value = 3.6514627716871804 W = [[0.40796644 0.52509596 1.06377296]] b = [0.74565448]\n",
      "step = 264000 loss value = 3.651374767735633 W = [[0.40795282 0.52508698 1.06380615]] b = [0.7447171]\n",
      "step = 265000 loss value = 3.651286855323358 W = [[0.40793921 0.525078   1.06383932]] b = [0.7437802]\n",
      "step = 266000 loss value = 3.651199034355241 W = [[0.40792561 0.52506903 1.06387248]] b = [0.74284379]\n",
      "step = 267000 loss value = 3.6511113047360677 W = [[0.40791201 0.52506006 1.06390562]] b = [0.74190787]\n",
      "step = 268000 loss value = 3.6510236663708904 W = [[0.40789842 0.5250511  1.06393874]] b = [0.74097244]\n",
      "step = 269000 loss value = 3.650936119164747 W = [[0.40788484 0.52504214 1.06397185]] b = [0.74003749]\n",
      "step = 270000 loss value = 3.6508486630228325 W = [[0.40787126 0.52503319 1.06400494]] b = [0.73910303]\n",
      "step = 271000 loss value = 3.650761297850436 W = [[0.4078577  0.52502424 1.06403801]] b = [0.73816905]\n",
      "step = 272000 loss value = 3.650674023552897 W = [[0.40784413 0.52501529 1.06407106]] b = [0.73723556]\n",
      "step = 273000 loss value = 3.6505868400357633 W = [[0.40783058 0.52500635 1.0641041 ]] b = [0.73630255]\n",
      "step = 274000 loss value = 3.6504997472045306 W = [[0.40781703 0.52499742 1.06413712]] b = [0.73537004]\n",
      "step = 275000 loss value = 3.6504127449648993 W = [[0.40780349 0.52498849 1.06417012]] b = [0.734438]\n",
      "step = 276000 loss value = 3.6503258332226216 W = [[0.40778996 0.52497956 1.0642031 ]] b = [0.73350645]\n",
      "step = 277000 loss value = 3.6502390118835897 W = [[0.40777644 0.52497064 1.06423607]] b = [0.73257539]\n",
      "step = 278000 loss value = 3.6501522808537255 W = [[0.40776292 0.52496173 1.06426902]] b = [0.73164481]\n",
      "step = 279000 loss value = 3.650065640039175 W = [[0.40774941 0.52495281 1.06430196]] b = [0.73071471]\n",
      "step = 280000 loss value = 3.649979089346035 W = [[0.4077359  0.52494391 1.06433487]] b = [0.7297851]\n",
      "step = 281000 loss value = 3.649892628680551 W = [[0.4077224  0.52493501 1.06436777]] b = [0.72885597]\n",
      "step = 282000 loss value = 3.649806257949155 W = [[0.40770891 0.52492611 1.06440065]] b = [0.72792733]\n",
      "step = 283000 loss value = 3.6497199770582167 W = [[0.40769543 0.52491721 1.06443352]] b = [0.72699916]\n",
      "step = 284000 loss value = 3.649633785914301 W = [[0.40768195 0.52490833 1.06446637]] b = [0.72607148]\n",
      "step = 285000 loss value = 3.6495476844241233 W = [[0.40766849 0.52489944 1.0644992 ]] b = [0.72514429]\n",
      "step = 286000 loss value = 3.6494616724943327 W = [[0.40765502 0.52489056 1.06453201]] b = [0.72421757]\n",
      "step = 287000 loss value = 3.64937575003185 W = [[0.40764157 0.52488169 1.06456481]] b = [0.72329134]\n",
      "step = 288000 loss value = 3.649289916943557 W = [[0.40762812 0.52487282 1.06459759]] b = [0.72236559]\n",
      "step = 289000 loss value = 3.649204173136543 W = [[0.40761468 0.52486395 1.06463035]] b = [0.72144032]\n",
      "step = 290000 loss value = 3.649118518517913 W = [[0.40760124 0.52485509 1.0646631 ]] b = [0.72051553]\n",
      "step = 291000 loss value = 3.6490329529948564 W = [[0.40758782 0.52484624 1.06469583]] b = [0.71959123]\n",
      "step = 292000 loss value = 3.6489474764747416 W = [[0.4075744  0.52483739 1.06472854]] b = [0.7186674]\n",
      "step = 293000 loss value = 3.648862088864964 W = [[0.40756098 0.52482854 1.06476123]] b = [0.71774406]\n",
      "step = 294000 loss value = 3.648776790073134 W = [[0.40754758 0.5248197  1.06479391]] b = [0.71682119]\n",
      "step = 295000 loss value = 3.6486915800067266 W = [[0.40753418 0.52481086 1.06482657]] b = [0.71589881]\n",
      "step = 296000 loss value = 3.648606458573531 W = [[0.40752078 0.52480203 1.06485921]] b = [0.7149769]\n",
      "step = 297000 loss value = 3.648521425681337 W = [[0.4075074  0.5247932  1.06489184]] b = [0.71405548]\n",
      "step = 298000 loss value = 3.6484364812380736 W = [[0.40749402 0.52478437 1.06492445]] b = [0.71313453]\n",
      "step = 299000 loss value = 3.6483516251517063 W = [[0.40748065 0.52477555 1.06495704]] b = [0.71221407]\n",
      "step = 300000 loss value = 3.6482668573303108 W = [[0.40746728 0.52476674 1.06498962]] b = [0.71129408]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 데이터 불러오기\n",
    "loaded_data = np.loadtxt(\"./data_01.csv\", delimiter=\",\", dtype=np.float32, skiprows=0)\n",
    "x_data = loaded_data[:, :-1]\n",
    "t_data = loaded_data[:, [-1]]\n",
    "\n",
    "# w와 b 초기화 (x_data의 열 수에 맞춰서 w 초기화)\n",
    "w = np.random.rand(x_data.shape[1], 1)  # (입력 차원, 1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "# 손실 함수 정의\n",
    "def loss_function(x, t):\n",
    "    y = np.dot(x, w) + b\n",
    "    return np.mean((t - y) ** 2)\n",
    "\n",
    "# 수치 미분 함수 정의\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=['readwrite'])\n",
    "\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x_data)\n",
    "\n",
    "        x[idx] = float(tmp_val) - delta_x\n",
    "        fx2 = f(x_data)\n",
    "\n",
    "        grad[idx] = (fx1 - fx2) / (2 * delta_x)\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "\n",
    "    return grad\n",
    "\n",
    "# 학습률과 학습 루프\n",
    "learning_rate = 1e-5\n",
    "f = lambda x: loss_function(x_data, t_data)\n",
    "\n",
    "print(\"Initial loss value =\", loss_function(x_data, t_data))\n",
    "\n",
    "for step in range(300001):\n",
    "    w -= learning_rate * numerical_derivative(f, w)\n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(\"step =\", step,\n",
    "              \"loss value =\", loss_function(x_data, t_data),\n",
    "              \"W =\", w.T,\n",
    "              \"b =\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ef6fdf4-41e7-465f-9d22-57c4c91cf8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([179.14932207])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([100, 98, 81])\n",
    "predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9ddb9-a8b7-4ca9-9e5d-466365d176c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
