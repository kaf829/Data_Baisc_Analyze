{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6515f985-5631-4ee0-a586-9034d48390ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89e16de4-c5d6-4300-94ee-1ffe3211a7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ea6abf-a7d7-4aa8-9f6e-6477ef29b341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1:  ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphange', 'is', 'a', 'cherry', 'as', 'cherry', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화1: ', word_tokenize(\"\"\"Don't be fooled by the dark sounding name,\n",
    "Mr. Jone's Orphange is a cherry as cherry goes for a pastry shop. \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5a609b7-8959-4e42-9f3d-cd4d9e1d3fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1:  ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphange', 'is', 'a', 'cherry', 'as', 'cherry', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화1: ', WordPunctTokenizer().tokenize(\"\"\"Don't be fooled by the dark sounding name,\n",
    "Mr. Jone's Orphange is a cherry as cherry goes for a pastry shop. \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d473acce-6642-4dac-94fc-b8ebf1404b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1:  [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphange', 'is', 'a', 'cherry', 'as', 'cherry', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화1: ', text_to_word_sequence(\"\"\"Don't be fooled by the dark sounding name,\n",
    "Mr. Jone's Orphange is a cherry as cherry goes for a pastry shop. \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f649e577-4846-45d2-8227-0db40af53578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리뱅크 워드토크나이저 ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(\"트리뱅크 워드토크나이저\", tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29a1bdb6-b5fe-4ac7-a9d7-ec20c69f7f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', ':', 'I', \"'m\", 'a', 'text-based', 'example', ',', 'rock-n-roll', ',', 'and', 'web', '2.0.', 'Hello', ',', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"This is an example: I'm a text-based example, rock-n-roll, and web 2.0. Hello, world!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4af2b0e0-b185-4e57-aa9c-2e796624d776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화1:  ['His barther kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'finally, the barther went up a mountain and almost to the edge of a cliff.', 'he dug a hole ing the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text =\"\"\"His barther kept his word. But keeping such a huge secret to himself was driving him crazy. finally, the barther went up a mountain and almost to the edge of a cliff. he dug a hole ing the midst of some reeds. He looked about, to make sure no one was near. \"\"\"\n",
    "print(\"문장 토큰화1: \", sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32ad1d7c-aec6-4f08-96df-3767bcadab7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장 토큰화 ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = \"딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?\"\n",
    "print(\"한국어 문장 토큰화\" , kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fde851d-9aa0-466a-a85d-422554a6dd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화: ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph', '..', 'D.', 'student', '.']\n",
      "품사 태깅:  [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph', 'NNP'), ('..', 'NNP'), ('D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text  = \"I am actively looking for Ph.D. students. and you are a Ph..D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('단어 토큰화:' , tokenized_sentence)\n",
    "print('품사 태깅: ' , pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "233296ac-1f65-4fee-8ba2-20cf57836d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석:  ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅:  [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출:  ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "\n",
    "print('OKT 형태소 분석: ', okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('OKT 품사 태깅: ', okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('OKT 명사 추출: ', okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "019df1f4-5e3e-4efa-be65-958dedd31c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "shortword = re.compile(r\"\\W*\\b\\w{1,2}\\b\")\n",
    "print(shortword.sub(\"\",text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b53b0b-0f54-4de2-b04a-f310a8c6afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organizaion', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "['policy', 'doing', 'organizaion', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['policy', 'doing', 'organizaion', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print(words)\n",
    "print([lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67f6342d-0451-4db8-b9b4-d25f775e2332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24c875de-a832-480a-bbc3-bf92f061a13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "495e84af-4b83-4419-983d-d9b7bb1ba2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e20c455a-0867-4e28-ac16-6879a425df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "print('어간 추출 전 :', tokenized_sentence)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b0bf613-1056-4181-bb04-e13eba49ffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['formalize', 'allowance', 'electricical']\n",
      "어간 추출 후 : ['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words = ['formalize', 'allowance', 'electricical']\n",
    "\n",
    "print('어간 추출 전 :',words)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9ca3890-c97c-4ef1-bff9-a58172834c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print('어간 추출 전 :', words)\n",
    "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
    "print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10cb70f3-993e-474c-bfb1-79e06df05616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ddd9908-fe2c-452b-9730-a08e36c3df1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 198\n",
      "불용어 10개 출력 : ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n"
     ]
    }
   ],
   "source": [
    "stop_words_list = stopwords.words('english')\n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 10개 출력 :',stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ba524ed-7a34-41df-a398-2f4011897582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words: \n",
    "        result.append(word) \n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9747b5c5-f93f-4994-9902-a390a9e78d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "r =  re.compile(\"a.c\")\n",
    "r.search(\"KKK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "912a5583-d4cd-42fe-87d2-b0221d7ca752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69efcf35-0562-4512-a186-389f08ba18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('ab?c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70561de3-46a3-4fdd-8915-598001018b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abbc')\n",
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1f1a615-d4e6-4710-bc71-3a22fd7cb1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('ac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "697fa1fa-c7d8-4275-a8b5-52c103197314",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile(\"ab{2,8}c\")\n",
    "r.search(\"ac\")\n",
    "r.search(\"abc\")\n",
    "r.search(\"abbbbbbbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0de9d46c-df5f-4f08-a30c-58794cf40f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f6a16ad-a148-4fdd-9936-3a2030145fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('[abc]')\n",
    "r.search('zzz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58c0d91d-1094-4b59-880b-a9c4678c8383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08b3f74e-3eba-4796-a66c-fa26e5989b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('aaaaaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "176fa50b-c872-40e9-9bb6-5ec13c98807e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='b'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('baac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86f6788d-ea04-4e40-af96-d994c7e20ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='d'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = re.compile('[^abc]')\n",
    "r.search('a')\n",
    "r.search('ab')\n",
    "r.search('b')\n",
    "r.search('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d41e88dc-c19f-4b06-b759-ad09c4a1e04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='d'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b49906c6-e533-4ec8-8260-24878cb55bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='1'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35f04842-9deb-4381-b0fb-e655354169cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='k'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.compile('ab.')\n",
    "r.match('kkkabc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f8db4eb-e43a-48af-9c43-830cff73df7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='k'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('kkkabc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "030d7e06-0752-43cf-9aae-eccb88f35131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 4), match='k'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abckkk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d489854-8161-4a45-a28b-104f3e4361ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.match('abckkk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4848cadb-da95-4ded-b0cd-64fbbe280fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 4), match='k'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abckkk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8369809-8829-42d9-a672-29ed0ec9293f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"사과 딸기 수박 메론 바나나\"\n",
    "re.split(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "316631b6-30a8-4119-a349-1ac5bd3b907e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"사과\n",
    "딸기\n",
    "수박\n",
    "메론\n",
    "바나나\"\"\"\n",
    "\n",
    "re.split(\"\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "857a1971-ef4b-4ca9-9f9c-0ae5a9f4526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"사과+딸기+수박+메론+바나나\"\n",
    "\n",
    "re.split(\"\\+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "930c76d7-b918-4c5f-ba42-217a4bf6bed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '1234', '30']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"이름: 김철수\n",
    "전화번호 : 010 - 1234 -1234\n",
    "나이 : 30\n",
    "성별: 남\"\"\"\n",
    "\n",
    "re.findall(\"\\d+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2032593e-827e-40ee-98a9-82b2f6fc631a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"\\d\", \"문자열입니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b327ed7b-1c85-406e-b980-7774cafe1ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern \n"
     ]
    }
   ],
   "source": [
    "text = \"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "\n",
    "preprocessed_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de2c1769-9065-4bd4-b309-a0e042c690ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"Hello, world! This is a test.\"\n",
    "patterm = \"world\"\n",
    "match = re.search(\"\\\\w+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d76535e8-3c7e-4c02-97ef-fa02934d64e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "매치된 부분: Hello\n",
      "시작 위치 0\n",
      "종료 위치 5\n"
     ]
    }
   ],
   "source": [
    "if match:\n",
    "    print(\"매치된 부분:\", match.group(0))\n",
    "    print(\"시작 위치\", match.start())\n",
    "    print(\"종료 위치\" ,match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ec2a0a7-e708-4d4d-b760-24707a496626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world! This is a test.\"\n",
    "pattern = \"world\"\n",
    "match = re.findall(\"\\\\w+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18d780aa-25cc-4312-84cb-113f22ac98a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'This', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26360766-f066-4195-a81f-cf55faaaee1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROF', 'STUD', 'STUD']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"100 John   PROF\n",
    "101 James STUD\n",
    "102 Mac STUD\"\"\"\n",
    "\n",
    "re.findall('[A-Z]{4}',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32e4c422-3db9-409a-a23e-c2baa256c5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James', 'Mac']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][a-z]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c147d0c2-7721-4374-970e-c9cab75479b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphange', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', \"Mr.Jone's\", 'Orphange', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer  # 정규표현식을 이용해 토큰화하는 클래스\n",
    "\n",
    "# 입력 텍스트\n",
    "text = \"Don't be fooled by the dark sounding name, Mr.Jone's Orphange is as cheery as cheery goes for a pastry shop\"\n",
    "\n",
    "# ✅ tokenizer1: 정규표현식 \"[\\w]+\" 사용 → 알파벳/숫자로 된 연속된 단어만 추출\n",
    "# \\w는 [a-zA-Z0-9_]와 같고, +는 하나 이상 연속된 경우를 의미\n",
    "# 따라서 구두점(’, . 등)은 제거되고 단어만 추출됨\n",
    "tokenizer1 = RegexpTokenizer(r\"[\\w]+\")\n",
    "\n",
    "# ✅ tokenizer2: 정규표현식 \"\\s+\" 사용, gaps=True → 공백(스페이스, 탭 등)을 기준으로 분리\n",
    "# gaps=True이면 '이 패턴에 맞는 부분'을 **구분자**로 사용 (split처럼 작동)\n",
    "tokenizer2 = RegexpTokenizer(r\"\\s+\", gaps=True)\n",
    "\n",
    "# ✅ tokenizer1 결과 출력\n",
    "# 단어 단위로만 나누며, 구두점과 특수문자 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a6efd-8d31-48df-b977-ac350f372461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
