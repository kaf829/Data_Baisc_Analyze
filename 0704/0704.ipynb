{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6515f985-5631-4ee0-a586-9034d48390ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89e16de4-c5d6-4300-94ee-1ffe3211a7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ea6abf-a7d7-4aa8-9f6e-6477ef29b341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1:  ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphange', 'is', 'a', 'cherry', 'as', 'cherry', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화1: ', word_tokenize(\"\"\"Don't be fooled by the dark sounding name,\n",
    "Mr. Jone's Orphange is a cherry as cherry goes for a pastry shop. \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5a609b7-8959-4e42-9f3d-cd4d9e1d3fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1:  ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphange', 'is', 'a', 'cherry', 'as', 'cherry', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화1: ', WordPunctTokenizer().tokenize(\"\"\"Don't be fooled by the dark sounding name,\n",
    "Mr. Jone's Orphange is a cherry as cherry goes for a pastry shop. \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d473acce-6642-4dac-94fc-b8ebf1404b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1:  [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphange', 'is', 'a', 'cherry', 'as', 'cherry', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화1: ', text_to_word_sequence(\"\"\"Don't be fooled by the dark sounding name,\n",
    "Mr. Jone's Orphange is a cherry as cherry goes for a pastry shop. \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f649e577-4846-45d2-8227-0db40af53578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리뱅크 워드토크나이저 ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(\"트리뱅크 워드토크나이저\", tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29a1bdb6-b5fe-4ac7-a9d7-ec20c69f7f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', ':', 'I', \"'m\", 'a', 'text-based', 'example', ',', 'rock-n-roll', ',', 'and', 'web', '2.0.', 'Hello', ',', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"This is an example: I'm a text-based example, rock-n-roll, and web 2.0. Hello, world!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4af2b0e0-b185-4e57-aa9c-2e796624d776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화1:  ['His barther kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'finally, the barther went up a mountain and almost to the edge of a cliff.', 'he dug a hole ing the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text =\"\"\"His barther kept his word. But keeping such a huge secret to himself was driving him crazy. finally, the barther went up a mountain and almost to the edge of a cliff. he dug a hole ing the midst of some reeds. He looked about, to make sure no one was near. \"\"\"\n",
    "print(\"문장 토큰화1: \", sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32ad1d7c-aec6-4f08-96df-3767bcadab7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장 토큰화 ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = \"딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?\"\n",
    "print(\"한국어 문장 토큰화\" , kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fde851d-9aa0-466a-a85d-422554a6dd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화: ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph', '..', 'D.', 'student', '.']\n",
      "품사 태깅:  [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph', 'NNP'), ('..', 'NNP'), ('D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text  = \"I am actively looking for Ph.D. students. and you are a Ph..D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('단어 토큰화:' , tokenized_sentence)\n",
    "print('품사 태깅: ' , pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "233296ac-1f65-4fee-8ba2-20cf57836d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석:  ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅:  [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출:  ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "\n",
    "print('OKT 형태소 분석: ', okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('OKT 품사 태깅: ', okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('OKT 명사 추출: ', okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "019df1f4-5e3e-4efa-be65-958dedd31c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "shortword = re.compile(r\"\\W*\\b\\w{1,2}\\b\")\n",
    "print(shortword.sub(\"\",text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b53b0b-0f54-4de2-b04a-f310a8c6afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organizaion', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "['policy', 'doing', 'organizaion', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['policy', 'doing', 'organizaion', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print(words)\n",
    "print([lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67f6342d-0451-4db8-b9b4-d25f775e2332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24c875de-a832-480a-bbc3-bf92f061a13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "495e84af-4b83-4419-983d-d9b7bb1ba2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e20c455a-0867-4e28-ac16-6879a425df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "print('어간 추출 전 :', tokenized_sentence)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b0bf613-1056-4181-bb04-e13eba49ffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['formalize', 'allowance', 'electricical']\n",
      "어간 추출 후 : ['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words = ['formalize', 'allowance', 'electricical']\n",
    "\n",
    "print('어간 추출 전 :',words)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9ca3890-c97c-4ef1-bff9-a58172834c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print('어간 추출 전 :', words)\n",
    "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
    "print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10cb70f3-993e-474c-bfb1-79e06df05616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ddd9908-fe2c-452b-9730-a08e36c3df1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 198\n",
      "불용어 10개 출력 : ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n"
     ]
    }
   ],
   "source": [
    "stop_words_list = stopwords.words('english')\n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 10개 출력 :',stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ba524ed-7a34-41df-a398-2f4011897582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words: \n",
    "        result.append(word) \n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9747b5c5-f93f-4994-9902-a390a9e78d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "r =  re.compile(\"a.c\")\n",
    "r.search(\"KKK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "912a5583-d4cd-42fe-87d2-b0221d7ca752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69efcf35-0562-4512-a186-389f08ba18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('ab?c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70561de3-46a3-4fdd-8915-598001018b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abbc')\n",
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1f1a615-d4e6-4710-bc71-3a22fd7cb1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('ac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "697fa1fa-c7d8-4275-a8b5-52c103197314",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile(\"ab{2,8}c\")\n",
    "r.search(\"ac\")\n",
    "r.search(\"abc\")\n",
    "r.search(\"abbbbbbbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0de9d46c-df5f-4f08-a30c-58794cf40f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f6a16ad-a148-4fdd-9936-3a2030145fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('[abc]')\n",
    "r.search('zzz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58c0d91d-1094-4b59-880b-a9c4678c8383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08b3f74e-3eba-4796-a66c-fa26e5989b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('aaaaaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "176fa50b-c872-40e9-9bb6-5ec13c98807e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='b'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('baac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86f6788d-ea04-4e40-af96-d994c7e20ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='d'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = re.compile('[^abc]')\n",
    "r.search('a')\n",
    "r.search('ab')\n",
    "r.search('b')\n",
    "r.search('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d41e88dc-c19f-4b06-b759-ad09c4a1e04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='d'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b49906c6-e533-4ec8-8260-24878cb55bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='1'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35f04842-9deb-4381-b0fb-e655354169cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='k'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.compile('ab.')\n",
    "r.match('kkkabc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f8db4eb-e43a-48af-9c43-830cff73df7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='k'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('kkkabc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "030d7e06-0752-43cf-9aae-eccb88f35131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 4), match='k'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abckkk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d489854-8161-4a45-a28b-104f3e4361ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.match('abckkk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4848cadb-da95-4ded-b0cd-64fbbe280fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 4), match='k'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abckkk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8369809-8829-42d9-a672-29ed0ec9293f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"사과 딸기 수박 메론 바나나\"\n",
    "re.split(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "316631b6-30a8-4119-a349-1ac5bd3b907e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"사과\n",
    "딸기\n",
    "수박\n",
    "메론\n",
    "바나나\"\"\"\n",
    "\n",
    "re.split(\"\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "857a1971-ef4b-4ca9-9f9c-0ae5a9f4526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"사과+딸기+수박+메론+바나나\"\n",
    "\n",
    "re.split(\"\\+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "930c76d7-b918-4c5f-ba42-217a4bf6bed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '1234', '30']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"이름: 김철수\n",
    "전화번호 : 010 - 1234 -1234\n",
    "나이 : 30\n",
    "성별: 남\"\"\"\n",
    "\n",
    "re.findall(\"\\d+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2032593e-827e-40ee-98a9-82b2f6fc631a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"\\d\", \"문자열입니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b327ed7b-1c85-406e-b980-7774cafe1ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern \n"
     ]
    }
   ],
   "source": [
    "text = \"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "\n",
    "preprocessed_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de2c1769-9065-4bd4-b309-a0e042c690ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"Hello, world! This is a test.\"\n",
    "patterm = \"world\"\n",
    "match = re.search(\"\\\\w+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d76535e8-3c7e-4c02-97ef-fa02934d64e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "매치된 부분: Hello\n",
      "시작 위치 0\n",
      "종료 위치 5\n"
     ]
    }
   ],
   "source": [
    "if match:\n",
    "    print(\"매치된 부분:\", match.group(0))\n",
    "    print(\"시작 위치\", match.start())\n",
    "    print(\"종료 위치\" ,match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ec2a0a7-e708-4d4d-b760-24707a496626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world! This is a test.\"\n",
    "pattern = \"world\"\n",
    "match = re.findall(\"\\\\w+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18d780aa-25cc-4312-84cb-113f22ac98a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'This', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26360766-f066-4195-a81f-cf55faaaee1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROF', 'STUD', 'STUD']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"100 John   PROF\n",
    "101 James STUD\n",
    "102 Mac STUD\"\"\"\n",
    "\n",
    "re.findall('[A-Z]{4}',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32e4c422-3db9-409a-a23e-c2baa256c5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James', 'Mac']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][a-z]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c147d0c2-7721-4374-970e-c9cab75479b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphange', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', \"Mr.Jone's\", 'Orphange', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer  # 정규표현식을 이용해 토큰화하는 클래스\n",
    "\n",
    "# 입력 텍스트\n",
    "text = \"Don't be fooled by the dark sounding name, Mr.Jone's Orphange is as cheery as cheery goes for a pastry shop\"\n",
    "\n",
    "# ✅ tokenizer1: 정규표현식 \"[\\w]+\" 사용 → 알파벳/숫자로 된 연속된 단어만 추출\n",
    "# \\w는 [a-zA-Z0-9_]와 같고, +는 하나 이상 연속된 경우를 의미\n",
    "# 따라서 구두점(’, . 등)은 제거되고 단어만 추출됨\n",
    "tokenizer1 = RegexpTokenizer(r\"[\\w]+\")\n",
    "\n",
    "# ✅ tokenizer2: 정규표현식 \"\\s+\" 사용, gaps=True → 공백(스페이스, 탭 등)을 기준으로 분리\n",
    "# gaps=True이면 '이 패턴에 맞는 부분'을 **구분자**로 사용 (split처럼 작동)\n",
    "tokenizer2 = RegexpTokenizer(r\"\\s+\", gaps=True)\n",
    "\n",
    "# ✅ tokenizer1 결과 출력\n",
    "# 단어 단위로만 나누며, 구두점과 특수문자 제거됨\n",
    "print(tokenizer1.tokenize(text))\n",
    "# 출력: ['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphange', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
    "\n",
    "# ✅ tokenizer2 결과 출력\n",
    "# 공백을 기준으로 단어 분리 → 구두점은 그대로 남아 있음\n",
    "print(tokenizer2.tokenize(text))\n",
    "# 출력: [\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', \"Mr.Jone's\", 'Orphange', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0b0a6efd-8d31-48df-b977-ac350f372461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()\n",
    "print(tokenizer.morphs(\"에이비식스 이대휘 1월 최애돌 기부 요정\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f56736a9-ee20-4365-87d9-1d245a702130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2016-10-20.txt', <http.client.HTTPMessage at 0x2156ef05750>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9099d397-b467-475f-8c70-562dd51d8c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "\n",
    "corpus = DoublespaceLineCorpus('2016-10-26.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f458d7b8-73b8-4d4a-96d0-7ab1a1666b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot access local variable 'f' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MINJUN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')  # 처음 한 번만 실행하면 됨\n",
    "\n",
    "i = 0\n",
    "for document in corpus:\n",
    "  if len(document) > 0:\n",
    "    print(document)\n",
    "    i = i+1\n",
    "  if i == 3:\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8ccde46c-cd9f-45de-91b3-ac03fbbce908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot access local variable 'f' where it is not associated with a value\n",
      "training was done. used memory 0.230 Gb\n",
      "all cohesion probabilities was computed. # words = 0\n",
      "all branching entropies was computed # words = 0\n",
      "all accessor variety was computed # words = 0\n"
     ]
    }
   ],
   "source": [
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ffcb734d-4d39-46a5-ae9a-c39f43e4e401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot access local variable 'f' where it is not associated with a value\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "__len__() should return >= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msoynlp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DoublespaceLineCorpus\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m corpus = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDoublespaceLineCorpus\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2016-10-26.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m문단 수:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(corpus))\n",
      "\u001b[31mValueError\u001b[39m: __len__() should return >= 0"
     ]
    }
   ],
   "source": [
    "from soynlp import DoublespaceLineCorpus\n",
    "\n",
    "corpus = list(DoublespaceLineCorpus(\"2016-10-26.txt\"))\n",
    "print(\"문단 수:\", len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f6fe9884-9868-4d0a-b8b2-5ca67102ee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot access local variable 'f' where it is not associated with a value\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "__len__() should return >= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msoynlp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MaxScoreTokenizer\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1. 코퍼스 로딩\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m corpus = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDoublespaceLineCorpus\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2016-10-26.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 2. 단어 점수 학습\u001b[39;00m\n\u001b[32m      9\u001b[39m word_extractor = WordExtractor()\n",
      "\u001b[31mValueError\u001b[39m: __len__() should return >= 0"
     ]
    }
   ],
   "source": [
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "# 1. 코퍼스 로딩\n",
    "corpus = list(DoublespaceLineCorpus('2016-10-26.txt'))\n",
    "\n",
    "# 2. 단어 점수 학습\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_score_table = word_extractor.extract()\n",
    "\n",
    "# 3. 응집력 점수 기준으로 필터링\n",
    "scores = {\n",
    "    word: score.cohesion_forward\n",
    "    for word, score in word_score_table.items()\n",
    "    if 0.3 < score.cohesion_forward < 0.95 and len(word) <= 5  # 너무 긴 복합어 방지\n",
    "}\n",
    "\n",
    "# 4. MaxScoreTokenizer 사용\n",
    "tokenizer = MaxScoreTokenizer(scores)\n",
    "\n",
    "# 5. 문장 토큰화\n",
    "sentence = \"국제사회와 우리의 노력들로 범죄를 척결하자\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "170306d4-f035-4996-aa35-4cc02a98a7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국제사회와우리의노력', '들로범죄를척결하자']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "maxscore_tokenizer.tokenize(\"국제사회와우리의노력들로범죄를척결하자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d81a4678-62fc-42f6-8ef9-99d2b5dc0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.normalizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2fdd740c-236f-426d-9a57-76e746c45142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n"
     ]
    }
   ],
   "source": [
    "print(emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠ', num_repeats=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a057bb24-cdfc-4616-adb4-36990fab72f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와하하핫\n",
      "와하하핫\n",
      "와하하핫\n"
     ]
    }
   ],
   "source": [
    "print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하하하하하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하하하하핫', num_repeats=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "606cd87c-06df-44da-a82b-12c19e64413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MINJUN\\Documents\\GitHub\\Data_Baisc_Analyze\\Data_Baisc_Analyze\\.venv\\Lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['은', '경이', '는', '사무실', '로', '갔습니다']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "twitter.morphs(\"은경이는 사무실로 갔습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "293e24ae-467e-4e7f-b6d3-7e20cb715068",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.add_dictionary('은경이', 'Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6bebd65b-9a9c-49f8-b797-3ea5ca8c0bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['은경이', '는', '사무실', '로', '갔습니다']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.morphs('은경이는 사무실로 갔습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391c7802-8ec7-430d-bd1b-90fd5375c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기 처리 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a6d434-3357-4351-9c64-df2e72097564",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8fdf65-1994-462f-b6ab-6e9ea0e39517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "김철수는극중두인격의사나이이광수역을맡았다.철수는한국유일의태권도전승자를가리는결전의날을앞두고10년간함께훈련한사형인유연재(김광수분)를찾으러속세로내려온인물이다.\n"
     ]
    }
   ],
   "source": [
    "new_sent = sent.replace(\" \", '') \n",
    "print(sent)\n",
    "print(\"-\" * 100)\n",
    "print(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1fb59-40fe-4364-a0b4-c34550b15fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5915d-3700-43ea-8c01-58ca62300fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ca347-4fde-40d8-aa21-6e30a1a4dce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
