import cv2
import numpy as np

# ğŸ“Œ ì˜ìƒ íŒŒì¼ ì—´ê¸°
VideoSignal = cv2.VideoCapture(0)

# ğŸ“Œ YOLOv3 ê°€ì¤‘ì¹˜ì™€ ì„¤ì • íŒŒì¼ ë¡œë“œ
YOLO_net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")

# ğŸ“Œ í´ë˜ìŠ¤ ì´ë¦„ (ì˜ˆ: person, car ë“±) ë¡œë“œ
classes = []
with open("yolo.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]

# ğŸ“Œ YOLO ë„¤íŠ¸ì›Œí¬ ë ˆì´ì–´ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
layer_names = YOLO_net.getLayerNames()

# ğŸ“Œ ì¶œë ¥ ë ˆì´ì–´ ì´ë¦„ ì¶”ì¶œ (YOLOëŠ” ë§ˆì§€ë§‰ 3ê°œ ë ˆì´ì–´ì—ì„œ ì¶œë ¥ ë°œìƒ)
output_layers = [layer_names[i - 1] for i in YOLO_net.getUnconnectedOutLayers().flatten()]
print(output_layers)

# ğŸ“Œ í´ë˜ìŠ¤ë³„ë¡œ ëœë¤ ìƒ‰ìƒ ì§€ì • (ë°•ìŠ¤ ìƒ‰)
colors = np.random.uniform(0, 255, size=(len(classes), 3))

while True:
    # ğŸ“Œ ë¹„ë””ì˜¤ í”„ë ˆì„ ì½ê¸°
    ret, frame = VideoSignal.read()
    if not ret:
        break  # ì˜ìƒ ëë‚˜ë©´ ì¢…ë£Œ

    # ğŸ“Œ í˜„ì¬ í”„ë ˆì„ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
    h, w, c = frame.shape

    # ğŸ“Œ YOLO ì…ë ¥ìš© blob ìƒì„±
    # ìŠ¤ì¼€ì¼ 0.00392, í¬ê¸° 416x416, BGR -> RGB ë³€í™˜ (swapRB=True), crop X
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)

    # ğŸ“Œ YOLO ë„¤íŠ¸ì›Œí¬ì— ì…ë ¥ ë°ì´í„° ì„¤ì •
    YOLO_net.setInput(blob)

    # ğŸ“Œ ìˆœë°©í–¥ ì‹¤í–‰ â†’ ì¶œë ¥ ë ˆì´ì–´ì—ì„œ ê²°ê³¼ ì¶”ì¶œ
    outs = YOLO_net.forward(output_layers)

    # ğŸ“Œ íƒì§€ ê²°ê³¼ ì´ˆê¸°í™”
    class_ids = []  # í´ë˜ìŠ¤ ID ë¦¬ìŠ¤íŠ¸
    confidences = []  # confidence score ë¦¬ìŠ¤íŠ¸
    boxes = []  # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸

    # ğŸ“Œ YOLO ì¶œë ¥ ê²°ê³¼ í•´ì„
    for out in outs:
        for detection in out:
            score = detection[5:]  # í´ë˜ìŠ¤ë³„ í™•ë¥  score ì¶”ì¶œ
            class_id = np.argmax(score)  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ID
            confidence = score[class_id]  # ê·¸ í´ë˜ìŠ¤ì˜ í™•ë¥  ê°’
            if confidence > 0.5:  # confidence ì„ê³„ê°’ ì´ìƒì¼ ë•Œë§Œ ì‚¬ìš©
                center_x = int(detection[0] * w)  # ì¤‘ì‹¬ x ì¢Œí‘œ (ì´ë¯¸ì§€ í¬ê¸° ë°˜ì˜)
                center_y = int(detection[1] * h)  # ì¤‘ì‹¬ y ì¢Œí‘œ
                box_w = int(detection[2] * w)  # ë°•ìŠ¤ ë„ˆë¹„
                box_h = int(detection[3] * h)  # ë°•ìŠ¤ ë†’ì´
                x = int(center_x - box_w / 2)  # ì¢Œìƒë‹¨ x ì¢Œí‘œ
                y = int(center_y - box_h / 2)  # ì¢Œìƒë‹¨ y ì¢Œí‘œ

                # ê²°ê³¼ ì €ì¥
                boxes.append([x, y, box_w, box_h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    # ğŸ“Œ NMS (Non-Maximum Suppression)ë¡œ ì¤‘ë³µ ë°•ìŠ¤ ì œê±°
    # confidence 0.5 ì´ìƒ, IoU 0.4 ì´ìƒì¸ ì¤‘ë³µ ë°•ìŠ¤ ì œê±°
    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    font = cv2.FONT_HERSHEY_PLAIN  # í…ìŠ¤íŠ¸ í°íŠ¸ ì§€ì •

    # ğŸ“Œ NMS ì ìš©ëœ ë°•ìŠ¤ë§Œ ê·¸ë¦¬ê¸°
    for i in range(len(boxes)):
        if i in indices:  # NMSì—ì„œ ì„ íƒëœ ë°•ìŠ¤ë§Œ ì²˜ë¦¬
            x, y, w_box, h_box = boxes[i]
            label = str(classes[class_ids[i]])  # í´ë˜ìŠ¤ëª…
            color = colors[class_ids[i]]  # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(frame, (x, y), (x + w_box, y + h_box), color, 2)
            # í´ë˜ìŠ¤ëª… + confidence í‘œì‹œ
            cv2.putText(frame, label, (x, y - 20), font, 0.5, color, 1)

    # ğŸ“Œ íƒì§€ ê²°ê³¼ í”„ë ˆì„ ì¶œë ¥
    cv2.imshow("YOLO Video Detection", frame)

    # ğŸ“Œ 100ms ê°„ ëŒ€ê¸°, í‚¤ ì…ë ¥ ì‹œ ë£¨í”„ ì¢…ë£Œ
    if cv2.waitKey(100) > 0:
        break

# ğŸ“Œ ìì› í•´ì œ
VideoSignal.release()
cv2.destroyAllWindows()
