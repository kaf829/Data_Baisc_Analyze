{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c28379-9317-4c4e-9a59-6e6da867a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c31cbf9a-8f93-4441-8c35-f20c6442abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60970fe0-3da9-4c64-9b09-460764c11163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivatice(f,x):\n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags =['multi_index'], op_flags = ['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x)\n",
    "\n",
    "        x[idx] = float(tmp_val) - delta_x\n",
    "        fx2 = f(x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a9fbe50-3e3a-4a9e-a145-0a90f1d04d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicGate:\n",
    "    def __init__(self, gate_name, xdata, tdata):\n",
    "        self.name = gate_name\n",
    "        self.xdata = xdata.reshape(4,2)\n",
    "        self.tdata = tdata.reshape(4,1)\n",
    "\n",
    "        self.W = np.random.rand(self.xdata.shape[1], 1)\n",
    "        self.b = np.random.rand(1)\n",
    "        self.learning_rate = 1e-2\n",
    "\n",
    "    def loss_func(self):\n",
    "        delta = 1e-7\n",
    "        z= np.dot(self.xdata, self.W) + self.b\n",
    "        y = sigmoid(z)\n",
    "        return -np.sum(self.tdata*np.log(y+delta) + (1-self.tdata)*np.log((1-y) + delta))\n",
    "\n",
    "    def train(self):\n",
    "        f = lambda x: self.loss_func()\n",
    "        print(\"Initial loss value = \" , self.loss_func())\n",
    "        for step in range(18001):\n",
    "            self.W -= self.learning_rate * numerical_derivatice(f, self.W)\n",
    "            self.b -= self.learning_rate * numerical_derivatice(f, self.b)\n",
    "            if(step % 1000 == 0):\n",
    "                print(\"step = \", step, \"loss value = \", self.loss_func())\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        z = np.dot(input_data, self.W) + self.b\n",
    "        y = sigmoid(z)\n",
    "        if y > 0.5:\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "        return y, result\n",
    "    def accuracy(self, test_xdata, test_tdata):\n",
    "        matched_list = []\n",
    "        not_matched_list = []\n",
    "        for index in range(len(xdata)):\n",
    "            (real_val, logical_val) = self.predict(test_xdata[index])\n",
    "            if logical_val == test_tdata[index]:\n",
    "                matched_list.append(index)\n",
    "            else:\n",
    "                not_matched_list.append(index)\n",
    "\n",
    "        accuracy_val = len(matched_list)/ len(test_xdata)\n",
    "        return accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b6c6616-489f-4a2b-805f-e74a70aa36be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value =  4.733588344955858\n",
      "step =  0 loss value =  4.672571372111963\n",
      "step =  1000 loss value =  1.004364805380484\n",
      "step =  2000 loss value =  0.6591682051600525\n",
      "step =  3000 loss value =  0.4908014401742059\n",
      "step =  4000 loss value =  0.3898822372286026\n",
      "step =  5000 loss value =  0.3226357949210963\n",
      "step =  6000 loss value =  0.2747062247490339\n",
      "step =  7000 loss value =  0.23888204814270783\n",
      "step =  8000 loss value =  0.21113423326359232\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "tdata = np.array([0,0,0,1])\n",
    "AND_obj = LogicGate(\"AND_GATE\", xdata, tdata)\n",
    "AND_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae1129c5-e9c7-49c5-a579-343416472a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]  =  0\n",
      "[0 1]  =  0\n",
      "[1 0]  =  0\n",
      "[1 1]  =  1\n",
      "---------------------\n",
      "Accuracy = > 1.0\n"
     ]
    }
   ],
   "source": [
    "test_xdata = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "for input_data in test_xdata:\n",
    "    (sigmoid_val, logical_val) = AND_obj.predict(input_data)\n",
    "    print(input_data, ' = ', logical_val)\n",
    "\n",
    "print(\"---------------------\")\n",
    "test_tdata = np.array([0,0,0,1])\n",
    "accuracy_ret = AND_obj.accuracy(test_xdata, test_tdata)\n",
    "print(\"Accuracy = >\" ,  accuracy_ret )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccfb858e-a8b1-4637-a065-7b46e5365909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 4.332356790391165\n",
      "step 1000, loss = 0.9777699236021804\n",
      "step 2000, loss = 0.6478735987840263\n",
      "step 3000, loss = 0.48444486021446553\n",
      "step 4000, loss = 0.38580330186327616\n",
      "step 5000, loss = 0.3198024660454012\n",
      "step 6000, loss = 0.27262821516097174\n",
      "step 7000, loss = 0.23729581356754603\n",
      "step 8000, loss = 0.2098855047333439\n",
      "step 9000, loss = 0.18802799979313384\n",
      "step 10000, loss = 0.17020819635788087\n",
      "step 11000, loss = 0.1554130964704313\n",
      "step 12000, loss = 0.14294040421312398\n",
      "step 13000, loss = 0.13228831948749686\n",
      "step 14000, loss = 0.1230890296769244\n",
      "step 15000, loss = 0.11506696336701802\n",
      "step 16000, loss = 0.1080116955116196\n",
      "step 17000, loss = 0.10175984701975407\n",
      "step 18000, loss = 0.09618268461025155\n",
      "step 19000, loss = 0.09117743522714403\n",
      "step 20000, loss = 0.08666108124521617\n",
      "step 21000, loss = 0.08256584910221354\n",
      "step 22000, loss = 0.07883587675419292\n",
      "step 23000, loss = 0.07542471634677575\n",
      "step 24000, loss = 0.07229343819626735\n",
      "step 25000, loss = 0.06940917403666721\n",
      "step 26000, loss = 0.06674398546298566\n",
      "step 27000, loss = 0.06427397608858668\n",
      "step 28000, loss = 0.061978588422876985\n",
      "step 29000, loss = 0.059840042224012194\n",
      "step 30000, loss = 0.05784288225866047\n",
      "step 31000, loss = 0.05597361143468586\n",
      "step 32000, loss = 0.054220391113218275\n",
      "step 33000, loss = 0.05257279470006506\n",
      "step 34000, loss = 0.05102160380390116\n",
      "step 35000, loss = 0.04955863863786615\n",
      "step 36000, loss = 0.04817661614792093\n",
      "step 37000, loss = 0.046869030729011114\n",
      "step 38000, loss = 0.04563005344911662\n",
      "step 39000, loss = 0.04445444652087181\n",
      "step 40000, loss = 0.043337490400022466\n",
      "step 41000, loss = 0.04227492139160086\n",
      "step 42000, loss = 0.0412628780411229\n",
      "step 43000, loss = 0.04029785490310479\n",
      "step 44000, loss = 0.03937666253082822\n",
      "step 45000, loss = 0.03849639273356599\n",
      "step 46000, loss = 0.03765438831077886\n",
      "step 47000, loss = 0.03684821660538445\n",
      "step 48000, loss = 0.036075646326259786\n",
      "step 49000, loss = 0.03533462717866827\n",
      "step 50000, loss = 0.03462327191415199\n",
      "step 51000, loss = 0.03393984047159996\n",
      "step 52000, loss = 0.03328272593104555\n",
      "step 53000, loss = 0.03265044204331099\n",
      "step 54000, loss = 0.03204161213329283\n",
      "step 55000, loss = 0.031454959203772455\n",
      "step 56000, loss = 0.030889297091101137\n",
      "step 57000, loss = 0.03034352254474451\n",
      "step 58000, loss = 0.029816608120188347\n",
      "step 59000, loss = 0.029307595789490676\n",
      "step 60000, loss = 0.028815591186447054\n",
      "step 61000, loss = 0.02833975841414546\n",
      "step 62000, loss = 0.02787931535184691\n",
      "step 63000, loss = 0.027433529406149387\n",
      "step 64000, loss = 0.0270017136581861\n",
      "step 65000, loss = 0.02658322336449323\n",
      "step 66000, loss = 0.02617745277427131\n",
      "step 67000, loss = 0.02578383223020977\n",
      "step 68000, loss = 0.025401825523872327\n",
      "step 69000, loss = 0.025030927479939097\n",
      "step 70000, loss = 0.02467066174659298\n",
      "step 71000, loss = 0.02432057877180096\n",
      "step 72000, loss = 0.023980253947535557\n",
      "step 73000, loss = 0.023649285905916355\n",
      "step 74000, loss = 0.023327294952955227\n",
      "step 75000, loss = 0.02301392162714399\n",
      "step 76000, loss = 0.022708825371429972\n",
      "step 77000, loss = 0.022411683308313946\n",
      "step 78000, loss = 0.022122189108874604\n",
      "step 79000, loss = 0.02184005194740254\n",
      "step 80000, loss = 0.02156499553421111\n",
      "step 81000, loss = 0.02129675721988118\n",
      "step 82000, loss = 0.0210350871648598\n",
      "step 83000, loss = 0.02077974756893279\n",
      "step 84000, loss = 0.020530511955604427\n",
      "step 85000, loss = 0.02028716450685359\n",
      "step 86000, loss = 0.02004949944419409\n",
      "step 87000, loss = 0.019817320452345286\n",
      "step 88000, loss = 0.01959044014208979\n",
      "step 89000, loss = 0.019368679549285124\n",
      "step 90000, loss = 0.01915186766720231\n",
      "step 91000, loss = 0.018939841009649263\n",
      "step 92000, loss = 0.018732443202529037\n",
      "step 93000, loss = 0.01852952460171114\n",
      "step 94000, loss = 0.01833094193524695\n",
      "step 95000, loss = 0.018136557968163016\n",
      "step 96000, loss = 0.017946241188159014\n",
      "step 97000, loss = 0.01775986551072451\n",
      "step 98000, loss = 0.01757731000230374\n",
      "step 99000, loss = 0.017398458620187757\n",
      "step 100000, loss = 0.017223199968024166\n",
      "step 101000, loss = 0.017051427065814084\n",
      "step 102000, loss = 0.01688303713344054\n",
      "step 103000, loss = 0.016717931386784683\n",
      "step 104000, loss = 0.016556014845603886\n",
      "step 105000, loss = 0.016397196152367094\n",
      "step 106000, loss = 0.016241387401353308\n",
      "step 107000, loss = 0.016088503977308946\n",
      "step 108000, loss = 0.015938464403071718\n",
      "step 109000, loss = 0.01579119019558741\n",
      "step 110000, loss = 0.01564660572975117\n",
      "step 111000, loss = 0.015504638109629806\n",
      "step 112000, loss = 0.015365217046566185\n",
      "step 113000, loss = 0.01522827474375825\n",
      "step 114000, loss = 0.015093745786910213\n",
      "step 115000, loss = 0.014961567040587362\n",
      "step 116000, loss = 0.014831677549928538\n",
      "step 117000, loss = 0.0147040184473977\n",
      "step 118000, loss = 0.014578532864288741\n",
      "step 119000, loss = 0.014455165846666364\n",
      "step 120000, loss = 0.014333864275538042\n",
      "step 121000, loss = 0.01421457679096285\n",
      "step 122000, loss = 0.014097253719901373\n",
      "step 123000, loss = 0.01398184700758669\n",
      "step 124000, loss = 0.013868310152216404\n",
      "step 125000, loss = 0.013756598142771657\n",
      "step 126000, loss = 0.013646667399809534\n",
      "step 127000, loss = 0.013538475719045762\n",
      "step 128000, loss = 0.01343198221758404\n",
      "step 129000, loss = 0.013327147282646327\n",
      "step 130000, loss = 0.01322393252267419\n",
      "step 131000, loss = 0.013122300720661275\n",
      "step 132000, loss = 0.01302221578962258\n",
      "step 133000, loss = 0.012923642730056332\n",
      "step 134000, loss = 0.012826547589327176\n",
      "step 135000, loss = 0.012730897422843782\n",
      "step 136000, loss = 0.012636660256957038\n",
      "step 137000, loss = 0.012543805053473859\n",
      "step 138000, loss = 0.012452301675733243\n",
      "step 139000, loss = 0.012362120856128068\n",
      "step 140000, loss = 0.01227323416503025\n",
      "step 141000, loss = 0.01218561398103896\n",
      "step 142000, loss = 0.012099233462478466\n",
      "step 143000, loss = 0.012014066520105903\n",
      "step 144000, loss = 0.011930087790943025\n",
      "step 145000, loss = 0.011847272613204667\n",
      "step 146000, loss = 0.011765597002246405\n",
      "step 147000, loss = 0.01168503762750126\n",
      "step 148000, loss = 0.011605571790358003\n",
      "step 149000, loss = 0.011527177402908918\n",
      "step 150000, loss = 0.011449832967580405\n",
      "step 151000, loss = 0.011373517557544163\n",
      "step 152000, loss = 0.011298210797921074\n",
      "step 153000, loss = 0.011223892847714394\n",
      "step 154000, loss = 0.01115054438244965\n",
      "step 155000, loss = 0.011078146577486306\n",
      "step 156000, loss = 0.01100668109197375\n",
      "step 157000, loss = 0.010936130053422357\n",
      "step 158000, loss = 0.01086647604285372\n",
      "step 159000, loss = 0.010797702080517321\n",
      "step 160000, loss = 0.010729791612148258\n",
      "step 161000, loss = 0.010662728495725115\n",
      "step 162000, loss = 0.010596496988729748\n",
      "step 163000, loss = 0.010531081735871093\n",
      "step 164000, loss = 0.01046646775726059\n",
      "step 165000, loss = 0.010402640437017968\n",
      "step 166000, loss = 0.010339585512286795\n",
      "step 167000, loss = 0.010277289062648349\n",
      "step 168000, loss = 0.010215737499912683\n",
      "step 169000, loss = 0.010154917558274128\n",
      "step 170000, loss = 0.010094816284811823\n",
      "step 171000, loss = 0.010035421030322709\n",
      "step 172000, loss = 0.009976719440481646\n",
      "step 173000, loss = 0.009918699447301654\n",
      "step 174000, loss = 0.009861349260892239\n",
      "step 175000, loss = 0.009804657361497575\n",
      "step 176000, loss = 0.009748612491814135\n",
      "step 177000, loss = 0.009693203649555846\n",
      "step 178000, loss = 0.009638420080285481\n",
      "step 179000, loss = 0.009584251270473645\n",
      "step 180000, loss = 0.009530686940797728\n",
      "[0 0] = 0\n",
      "[0 1] = 0\n",
      "[1 0] = 0\n",
      "[1 1] = 1\n",
      "---------------------\n",
      "Accuracy => 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # numpy 라이브러리 (배열, 수치 연산)\n",
    "\n",
    "# 시그모이드 함수 정의: 0~1 사이 값으로 변환\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "# 수치 미분 함수 (중심차분법 사용)\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4  # 작은 변화량\n",
    "    grad = np.zeros_like(x)  # 기울기 저장 배열\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])  # 인덱스 순회\n",
    "\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index  # 현재 위치 인덱스\n",
    "        tmp_val = x[idx]  # 현재 값 저장\n",
    "\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x)  # f(x + delta_x)\n",
    "\n",
    "        x[idx] = float(tmp_val) - delta_x\n",
    "        fx2 = f(x)  # f(x - delta_x)\n",
    "\n",
    "        grad[idx] = (fx1 - fx2) / (2 * delta_x)  # 중심차분 기울기\n",
    "        x[idx] = tmp_val  # 값 원래대로\n",
    "\n",
    "        it.iternext()  # 다음 인덱스로 이동\n",
    "\n",
    "    return grad  # 최종 기울기 반환\n",
    "\n",
    "# LogicGate 클래스 (퍼셉트론 모델)\n",
    "class LogicGate:\n",
    "    def __init__(self, gate_name, xdata, tdata):\n",
    "        self.name = gate_name  # 게이트 이름\n",
    "        self.xdata = xdata  # 입력 데이터\n",
    "        self.tdata = tdata.reshape(-1, 1)  # 출력 데이터 (열벡터)\n",
    "        self.W = np.random.rand(xdata.shape[1], 1)  # 가중치 초기화\n",
    "        self.b = np.random.rand(1)  # 바이어스 초기화\n",
    "        self.learning_rate = 1e-2  # 학습률\n",
    "\n",
    "    # 손실 함수 (이진 크로스 엔트로피)\n",
    "    def loss_func(self):\n",
    "        delta = 1e-7  # log 0 방지\n",
    "        z = np.dot(self.xdata, self.W) + self.b  # 선형 결합\n",
    "        y = sigmoid(z)  # 시그모이드 출력\n",
    "        return -np.sum(self.tdata * np.log(y + delta) + (1 - self.tdata) * np.log(1 - y + delta))\n",
    "\n",
    "    # 학습 함수 (수치 미분으로 최적화)\n",
    "    def train(self):\n",
    "        f = lambda x: self.loss_func()\n",
    "        for step in range(180001):\n",
    "            self.W -= self.learning_rate * numerical_derivative(f, self.W)\n",
    "            self.b -= self.learning_rate * numerical_derivative(f, self.b)\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                print(f\"step {step}, loss = {self.loss_func()}\")\n",
    "\n",
    "    # 예측 함수\n",
    "    def predict(self, x):\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        y = sigmoid(z)\n",
    "        logical_val = 1 if y > 0.5 else 0\n",
    "        return y, logical_val\n",
    "\n",
    "    # 정확도 계산 함수\n",
    "    def accuracy(self, test_input, test_target):\n",
    "        matched = 0\n",
    "        for idx in range(len(test_input)):\n",
    "            (_, logical_val) = self.predict(test_input[idx])\n",
    "            if logical_val == test_target[idx]:\n",
    "                matched += 1\n",
    "        return matched / len(test_input)\n",
    "\n",
    "# 입력과 출력 데이터 (AND 게이트)\n",
    "xdata = np.array([[0,0], [0,1], [1,0], [1,1]])  # 입력\n",
    "tdata = np.array([0,0,0,1])  # 출력\n",
    "\n",
    "# LogicGate 객체 생성 및 학습\n",
    "AND_obj = LogicGate(\"AND_GATE\", xdata, tdata)\n",
    "AND_obj.train()\n",
    "\n",
    "# 학습된 모델로 테스트\n",
    "test_xdata = np.array([[0,0], [0,1], [1,0], [1,1]])  # 테스트 데이터\n",
    "for input_data in test_xdata:\n",
    "    (sigmoid_val, logical_val) = AND_obj.predict(input_data)\n",
    "    print(f\"{input_data} = {logical_val}\")\n",
    "\n",
    "# 정확도 출력\n",
    "print(\"---------------------\")\n",
    "test_tdata = np.array([0,0,0,1])\n",
    "accuracy_ret = AND_obj.accuracy(test_xdata, test_tdata)\n",
    "print(\"Accuracy =>\", accuracy_ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6e23469-9e56-4652-aa56-a6dba192b464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 1.565368169153061\n",
      "step 1000, loss = 0.6718674371342357\n",
      "step 2000, loss = 0.41296805987246155\n",
      "step 3000, loss = 0.2939882232919356\n",
      "step 4000, loss = 0.22673829441799814\n",
      "step 5000, loss = 0.1838833694994694\n",
      "step 6000, loss = 0.15433633474280034\n",
      "step 7000, loss = 0.13279832383721724\n",
      "step 8000, loss = 0.11643483100915351\n",
      "step 9000, loss = 0.10359897103828389\n",
      "[0 0] =  0\n",
      "[0 1] =  1\n",
      "[1 0] =  1\n",
      "[1 1] =  1\n",
      "----------------------\n",
      "Accuracy =>  1.0\n"
     ]
    }
   ],
   "source": [
    "xdata =np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "tdaata = np.array([0,1,1,1])\n",
    "OR_obj = LogicGate(\"OR_GATE\", xdata, tdata)\n",
    "OR_obj.train()\n",
    "\n",
    "test_xdata= np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "for input_data in test_xdata:\n",
    "    (sigmoid_val, logical_val) = OR_obj.predict(input_data)\n",
    "    print(input_data, '= ', logical_val)\n",
    "\n",
    "\n",
    "print(\"----------------------\")\n",
    "test_xdata= np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "test_tdata = np.array([0,1,1,1])\n",
    "accuracy_ret = OR_obj.accuracy(test_xdata, test_tdata)\n",
    "print(\"Accuracy => \", accuracy_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ef30c68-8555-42f1-9b06-e0a659c63815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 2.9377464731492102\n",
      "step 1000, loss = 1.0795625811214387\n",
      "step 2000, loss = 0.6896645033699986\n",
      "step 3000, loss = 0.50767760106996\n",
      "step 4000, loss = 0.4006127115845902\n",
      "step 5000, loss = 0.33004524760621007\n",
      "step 6000, loss = 0.28011745945224725\n",
      "step 7000, loss = 0.2429995060398636\n",
      "step 8000, loss = 0.21436751561331796\n",
      "step 9000, loss = 0.19163868352520136\n",
      "step 10000, loss = 0.17317643455945042\n",
      "step 11000, loss = 0.15789456324874707\n",
      "step 12000, loss = 0.14504455592701615\n",
      "step 13000, loss = 0.13409429182772253\n",
      "step 14000, loss = 0.12465541277329552\n",
      "step 15000, loss = 0.11643803492508895\n",
      "step 16000, loss = 0.10922152118196449\n",
      "step 17000, loss = 0.10283505170961718\n",
      "step 18000, loss = 0.09714437048677597\n",
      "step 19000, loss = 0.09204253730348205\n",
      "step 20000, loss = 0.08744334355807344\n",
      "step 21000, loss = 0.0832765395695613\n",
      "step 22000, loss = 0.07948431865752445\n",
      "step 23000, loss = 0.076018688944359\n",
      "step 24000, loss = 0.07283948249726796\n",
      "step 25000, loss = 0.06991282887806721\n",
      "step 26000, loss = 0.06720997170532361\n",
      "step 27000, loss = 0.06470634173397836\n",
      "step 28000, loss = 0.062380823976995695\n",
      "step 29000, loss = 0.06021517317042056\n",
      "step 30000, loss = 0.058193543762925744\n",
      "step 31000, loss = 0.05630210913059208\n",
      "step 32000, loss = 0.05452875089925871\n",
      "step 33000, loss = 0.052862803792019075\n",
      "step 34000, loss = 0.05129484478040121\n",
      "step 35000, loss = 0.0498165178331338\n",
      "step 36000, loss = 0.04842038745517738\n",
      "step 37000, loss = 0.047099815655833877\n",
      "step 38000, loss = 0.04584885809436975\n",
      "step 39000, loss = 0.04466217600984014\n",
      "step 40000, loss = 0.043534961210180745\n",
      "step 41000, loss = 0.042462871919476286\n",
      "step 42000, loss = 0.041441977695793365\n",
      "step 43000, loss = 0.04046871196019057\n",
      "step 44000, loss = 0.03953983093941019\n",
      "step 45000, loss = 0.038652378035072724\n",
      "step 46000, loss = 0.037803652801952886\n",
      "step 47000, loss = 0.036991183855423306\n",
      "step 48000, loss = 0.036212705140327386\n",
      "step 49000, loss = 0.035466135085229306\n",
      "step 50000, loss = 0.034749558241490085\n",
      "step 51000, loss = 0.03406120906878125\n",
      "step 52000, loss = 0.03339945758033331\n",
      "step 53000, loss = 0.03276279660401239\n",
      "step 54000, loss = 0.03214983045124028\n",
      "step 55000, loss = 0.0315592648157714\n",
      "step 56000, loss = 0.030989897749528893\n",
      "step 57000, loss = 0.03044061158404412\n",
      "step 58000, loss = 0.02991036568401665\n",
      "step 59000, loss = 0.029398189934820608\n",
      "step 60000, loss = 0.02890317887876448\n",
      "step 61000, loss = 0.028424486426026352\n",
      "step 62000, loss = 0.027961321075713814\n",
      "step 63000, loss = 0.027512941590573423\n",
      "step 64000, loss = 0.02707865307600826\n",
      "step 65000, loss = 0.026657803419991945\n",
      "step 66000, loss = 0.026249780055776786\n",
      "step 67000, loss = 0.025854007013790883\n",
      "step 68000, loss = 0.025469942233051872\n",
      "step 69000, loss = 0.025097075105870028\n",
      "step 70000, loss = 0.024734924232594782\n",
      "step 71000, loss = 0.024383035365718876\n",
      "step 72000, loss = 0.02404097952503465\n",
      "step 73000, loss = 0.02370835126745912\n",
      "step 74000, loss = 0.02338476709692714\n",
      "step 75000, loss = 0.0230698640013228\n",
      "step 76000, loss = 0.02276329810478369\n",
      "step 77000, loss = 0.02246474342490459\n",
      "step 78000, loss = 0.02217389072542783\n",
      "step 79000, loss = 0.021890446456015657\n",
      "step 80000, loss = 0.02161413177146071\n",
      "step 81000, loss = 0.02134468162349885\n",
      "step 82000, loss = 0.021081843919048153\n",
      "step 83000, loss = 0.020825378739246333\n",
      "step 84000, loss = 0.02057505761426332\n",
      "step 85000, loss = 0.02033066284928088\n",
      "step 86000, loss = 0.02009198689749255\n",
      "step 87000, loss = 0.019858831776320786\n",
      "step 88000, loss = 0.01963100852345218\n",
      "step 89000, loss = 0.01940833668952912\n",
      "step 90000, loss = 0.019190643864681195\n",
      "step 91000, loss = 0.018977765236253205\n",
      "step 92000, loss = 0.018769543175411428\n",
      "step 93000, loss = 0.018565826850403797\n",
      "step 94000, loss = 0.018366471864523903\n",
      "step 95000, loss = 0.01817133991695312\n",
      "step 96000, loss = 0.017980298484794224\n",
      "step 97000, loss = 0.017793220524801784\n",
      "step 98000, loss = 0.01760998419337256\n",
      "step 99000, loss = 0.01743047258351822\n",
      "step 100000, loss = 0.017254573477631388\n",
      "step 101000, loss = 0.01708217911494752\n",
      "step 102000, loss = 0.016913185972702895\n",
      "step 103000, loss = 0.01674749456003694\n",
      "step 104000, loss = 0.01658500922380856\n",
      "step 105000, loss = 0.016425637965521316\n",
      "step 106000, loss = 0.01626929226859373\n",
      "step 107000, loss = 0.016115886935365732\n",
      "step 108000, loss = 0.0159653399331334\n",
      "step 109000, loss = 0.01581757224869006\n",
      "step 110000, loss = 0.015672507750801916\n",
      "step 111000, loss = 0.01553007306012498\n",
      "step 112000, loss = 0.01539019742611067\n",
      "step 113000, loss = 0.015252812610429461\n",
      "step 114000, loss = 0.015117852776570337\n",
      "step 115000, loss = 0.014985254385181692\n",
      "step 116000, loss = 0.014854956094844343\n",
      "step 117000, loss = 0.014726898667941975\n",
      "step 118000, loss = 0.014601024881315549\n",
      "step 119000, loss = 0.014477279441439558\n",
      "step 120000, loss = 0.014355608903834692\n",
      "step 121000, loss = 0.014235961596488664\n",
      "step 122000, loss = 0.014118287547053241\n",
      "step 123000, loss = 0.014002538413587472\n",
      "step 124000, loss = 0.013888667418661143\n",
      "step 125000, loss = 0.013776629286633237\n",
      "step 126000, loss = 0.013666380183910254\n",
      "step 127000, loss = 0.013557877662043206\n",
      "step 128000, loss = 0.013451080603485651\n",
      "step 129000, loss = 0.013345949169886222\n",
      "step 130000, loss = 0.013242444752768784\n",
      "step 131000, loss = 0.013140529926474558\n",
      "step 132000, loss = 0.013040168403250854\n",
      "step 133000, loss = 0.012941324990363061\n",
      "step 134000, loss = 0.012843965549136367\n",
      "step 135000, loss = 0.012748056955823001\n",
      "step 136000, loss = 0.012653567064184208\n",
      "step 137000, loss = 0.012560464669731285\n",
      "step 138000, loss = 0.01246871947551831\n",
      "step 139000, loss = 0.012378302059410179\n",
      "step 140000, loss = 0.012289183842763549\n",
      "step 141000, loss = 0.012201337060432985\n",
      "step 142000, loss = 0.012114734732055727\n",
      "step 143000, loss = 0.012029350634540255\n",
      "step 144000, loss = 0.011945159275703056\n",
      "step 145000, loss = 0.011862135869000586\n",
      "step 146000, loss = 0.011780256309298652\n",
      "step 147000, loss = 0.011699497149630249\n",
      "step 148000, loss = 0.0116198355789125\n",
      "step 149000, loss = 0.011541249400537149\n",
      "step 150000, loss = 0.011463717011844229\n",
      "step 151000, loss = 0.011387217384399764\n",
      "step 152000, loss = 0.011311730045051134\n",
      "step 153000, loss = 0.01123723505774278\n",
      "step 154000, loss = 0.011163713006018853\n",
      "step 155000, loss = 0.011091144976224973\n",
      "step 156000, loss = 0.011019512541345013\n",
      "step 157000, loss = 0.010948797745462407\n",
      "step 158000, loss = 0.010878983088811262\n",
      "step 159000, loss = 0.010810051513394297\n",
      "step 160000, loss = 0.010741986389139336\n",
      "step 161000, loss = 0.010674771500570931\n",
      "step 162000, loss = 0.010608391033978638\n",
      "step 163000, loss = 0.010542829565067975\n",
      "step 164000, loss = 0.01047807204704418\n",
      "step 165000, loss = 0.01041410379914446\n",
      "step 166000, loss = 0.010350910495586431\n",
      "step 167000, loss = 0.010288478154909832\n",
      "step 168000, loss = 0.01022679312969316\n",
      "step 169000, loss = 0.010165842096653187\n",
      "step 170000, loss = 0.010105612047079224\n",
      "step 171000, loss = 0.010046090277610796\n",
      "step 172000, loss = 0.00998726438133708\n",
      "step 173000, loss = 0.00992912223920433\n",
      "step 174000, loss = 0.009871652011722173\n",
      "step 175000, loss = 0.009814842130952432\n",
      "step 176000, loss = 0.009758681292774816\n",
      "step 177000, loss = 0.009703158449410623\n",
      "step 178000, loss = 0.009648262802206141\n",
      "step 179000, loss = 0.0095939837946497\n",
      "step 180000, loss = 0.009540311105627617\n",
      "[0 0] =  1\n",
      "[0 1] =  1\n",
      "[1 0] =  1\n",
      "[1 1] =  0\n",
      "----------------------\n",
      "Accuracy =>  1.0\n"
     ]
    }
   ],
   "source": [
    "# NAND 정답 데이터를 올바르게 지정\n",
    "xdata = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "tdata = np.array([1,1,1,0])  # NAND 출력\n",
    "NAND_obj = LogicGate(\"NAND_GATE\", xdata, tdata)\n",
    "NAND_obj.train()\n",
    "\n",
    "test_xdata = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "test_tdata = np.array([1,1,1,0])  # NAND 정답\n",
    "\n",
    "for input_data in test_xdata:\n",
    "    (sigmoid_val, logical_val) = NAND_obj.predict(input_data)\n",
    "    print(input_data, '= ', logical_val)\n",
    "\n",
    "print(\"----------------------\")\n",
    "accuracy_ret = NAND_obj.accuracy(test_xdata, test_tdata)\n",
    "print(\"Accuracy => \", accuracy_ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70e71cca-5aaf-4d30-9579-2be8fa96df47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 0.057992020503366146\n",
      "step 1000, loss = 0.05611333846140278\n",
      "step 2000, loss = 0.054351565823794856\n",
      "step 3000, loss = 0.05269617495651741\n",
      "step 4000, loss = 0.05113786081963722\n",
      "step 5000, loss = 0.04966836898279875\n",
      "step 6000, loss = 0.048280351815828224\n",
      "step 7000, loss = 0.04696724762181264\n",
      "step 8000, loss = 0.0457231785605152\n",
      "step 9000, loss = 0.044542864045905003\n",
      "[0 0] =  1\n",
      "[0 1] =  1\n",
      "[1 0] =  1\n",
      "[1 1] =  0\n",
      "----------------------\n",
      "Accuracy =>  0.75\n"
     ]
    }
   ],
   "source": [
    "# NAND 정답 데이터를 올바르게 지정\n",
    "xdata = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "tdata = np.array([1,1,1,0])  # NAND 출력\n",
    "XOR_obj = LogicGate(\"XOR_GATE\", xdata, tdata)\n",
    "NAND_obj.train()\n",
    "\n",
    "test_xdata = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "test_tdata = np.array([0,1,1,0]) \n",
    "\n",
    "for input_data in test_xdata:\n",
    "    (sigmoid_val, logical_val) = NAND_obj.predict(input_data)\n",
    "    print(input_data, '= ', logical_val)\n",
    "\n",
    "print(\"----------------------\")\n",
    "accuracy_ret = NAND_obj.accuracy(test_xdata, test_tdata)\n",
    "print(\"Accuracy => \", accuracy_ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9941cbe-035e-470f-ab57-7ec62b1620b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce7361-8452-4929-83de-8ad234cee7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
